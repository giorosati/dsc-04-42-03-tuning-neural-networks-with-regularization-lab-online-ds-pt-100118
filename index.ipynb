{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)\n",
    "\n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one line version\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, \n",
    "                                                    product_onehot, \n",
    "                                                    test_size=1500, \n",
    "                                                    random_state=42)\n",
    "\n",
    "# seperate line for each version\n",
    "# test_index = random.sample(range(1,10000), 1500)\n",
    "# test = one_hot_results[test_index]\n",
    "# train = np.delete(one_hot_results, test_index, 0)\n",
    "# label_test = product_onehot[test_index]\n",
    "# label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 102us/step - loss: 1.9524 - acc: 0.1560 - val_loss: 1.9438 - val_acc: 0.1640\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9276 - acc: 0.1851 - val_loss: 1.9278 - val_acc: 0.1860\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.9126 - acc: 0.2083 - val_loss: 1.9138 - val_acc: 0.2110\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8979 - acc: 0.2293 - val_loss: 1.8988 - val_acc: 0.2250\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8821 - acc: 0.2472 - val_loss: 1.8822 - val_acc: 0.2440\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8647 - acc: 0.2679 - val_loss: 1.8637 - val_acc: 0.2780\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8442 - acc: 0.2865 - val_loss: 1.8425 - val_acc: 0.2990\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8203 - acc: 0.3131 - val_loss: 1.8177 - val_acc: 0.3240\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7929 - acc: 0.3379 - val_loss: 1.7891 - val_acc: 0.3490\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7614 - acc: 0.3653 - val_loss: 1.7567 - val_acc: 0.3830\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7254 - acc: 0.4029 - val_loss: 1.7204 - val_acc: 0.4090\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6854 - acc: 0.4291 - val_loss: 1.6801 - val_acc: 0.4290\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6419 - acc: 0.4621 - val_loss: 1.6350 - val_acc: 0.4480\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5950 - acc: 0.4881 - val_loss: 1.5883 - val_acc: 0.4660\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5451 - acc: 0.5103 - val_loss: 1.5388 - val_acc: 0.4980\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4929 - acc: 0.5363 - val_loss: 1.4886 - val_acc: 0.5180\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4400 - acc: 0.5563 - val_loss: 1.4346 - val_acc: 0.5480\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3860 - acc: 0.5791 - val_loss: 1.3812 - val_acc: 0.5580\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3319 - acc: 0.5995 - val_loss: 1.3280 - val_acc: 0.5880\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2790 - acc: 0.6151 - val_loss: 1.2786 - val_acc: 0.6110\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.2277 - acc: 0.6303 - val_loss: 1.2277 - val_acc: 0.6320\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1789 - acc: 0.6471 - val_loss: 1.1821 - val_acc: 0.6380\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1329 - acc: 0.6631 - val_loss: 1.1349 - val_acc: 0.6450\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0899 - acc: 0.6731 - val_loss: 1.0949 - val_acc: 0.6560\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0499 - acc: 0.6840 - val_loss: 1.0608 - val_acc: 0.6660\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0131 - acc: 0.6907 - val_loss: 1.0239 - val_acc: 0.6720\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9784 - acc: 0.6969 - val_loss: 0.9925 - val_acc: 0.6750\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9473 - acc: 0.7067 - val_loss: 0.9630 - val_acc: 0.6830\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9180 - acc: 0.7112 - val_loss: 0.9375 - val_acc: 0.6880\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8911 - acc: 0.7196 - val_loss: 0.9133 - val_acc: 0.6950\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8665 - acc: 0.7235 - val_loss: 0.8903 - val_acc: 0.6960\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8433 - acc: 0.7301 - val_loss: 0.8705 - val_acc: 0.7060\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8220 - acc: 0.7364 - val_loss: 0.8502 - val_acc: 0.7040\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8024 - acc: 0.7416 - val_loss: 0.8359 - val_acc: 0.7110\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7837 - acc: 0.7424 - val_loss: 0.8181 - val_acc: 0.7120\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7661 - acc: 0.7493 - val_loss: 0.8062 - val_acc: 0.7150\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7507 - acc: 0.7528 - val_loss: 0.7901 - val_acc: 0.7270\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7356 - acc: 0.7571 - val_loss: 0.7762 - val_acc: 0.7280\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7214 - acc: 0.7608 - val_loss: 0.7649 - val_acc: 0.7320\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7082 - acc: 0.7639 - val_loss: 0.7554 - val_acc: 0.7370\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6956 - acc: 0.7680 - val_loss: 0.7452 - val_acc: 0.7350\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6840 - acc: 0.7709 - val_loss: 0.7357 - val_acc: 0.7360\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6728 - acc: 0.7744 - val_loss: 0.7267 - val_acc: 0.7360\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6624 - acc: 0.7755 - val_loss: 0.7198 - val_acc: 0.7400\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6522 - acc: 0.7800 - val_loss: 0.7134 - val_acc: 0.7310\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6427 - acc: 0.7811 - val_loss: 0.7042 - val_acc: 0.7430\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6339 - acc: 0.7840 - val_loss: 0.7004 - val_acc: 0.7530\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6254 - acc: 0.7863 - val_loss: 0.6933 - val_acc: 0.7500\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6172 - acc: 0.7887 - val_loss: 0.6884 - val_acc: 0.7410\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6089 - acc: 0.7908 - val_loss: 0.6824 - val_acc: 0.7510\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6017 - acc: 0.7957 - val_loss: 0.6769 - val_acc: 0.7520\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5946 - acc: 0.7951 - val_loss: 0.6722 - val_acc: 0.7520\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5877 - acc: 0.7979 - val_loss: 0.6683 - val_acc: 0.7500\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5803 - acc: 0.8040 - val_loss: 0.6640 - val_acc: 0.7520\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5740 - acc: 0.8031 - val_loss: 0.6602 - val_acc: 0.7560\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5672 - acc: 0.8064 - val_loss: 0.6569 - val_acc: 0.7530\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5615 - acc: 0.8087 - val_loss: 0.6551 - val_acc: 0.7520\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5558 - acc: 0.8087 - val_loss: 0.6509 - val_acc: 0.7560\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5500 - acc: 0.8107 - val_loss: 0.6449 - val_acc: 0.7590\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5447 - acc: 0.8148 - val_loss: 0.6415 - val_acc: 0.7590\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5388 - acc: 0.8168 - val_loss: 0.6390 - val_acc: 0.7570\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5338 - acc: 0.8188 - val_loss: 0.6361 - val_acc: 0.7580\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5279 - acc: 0.8215 - val_loss: 0.6347 - val_acc: 0.7600\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5233 - acc: 0.8249 - val_loss: 0.6316 - val_acc: 0.7640\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5178 - acc: 0.8248 - val_loss: 0.6293 - val_acc: 0.7660\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5138 - acc: 0.8259 - val_loss: 0.6265 - val_acc: 0.7620\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5090 - acc: 0.8281 - val_loss: 0.6253 - val_acc: 0.7610\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5043 - acc: 0.8293 - val_loss: 0.6226 - val_acc: 0.7630\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4998 - acc: 0.8311 - val_loss: 0.6196 - val_acc: 0.7680\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.4955 - acc: 0.8327 - val_loss: 0.6189 - val_acc: 0.7670\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4909 - acc: 0.8341 - val_loss: 0.6199 - val_acc: 0.7670\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4871 - acc: 0.8372 - val_loss: 0.6159 - val_acc: 0.7690\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.4830 - acc: 0.8376 - val_loss: 0.6134 - val_acc: 0.7690\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4784 - acc: 0.8392 - val_loss: 0.6120 - val_acc: 0.7680\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.4746 - acc: 0.8388 - val_loss: 0.6106 - val_acc: 0.7660\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.4707 - acc: 0.8421 - val_loss: 0.6094 - val_acc: 0.7660\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.4668 - acc: 0.8419 - val_loss: 0.6067 - val_acc: 0.7680\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 1s 103us/step - loss: 0.4628 - acc: 0.8431 - val_loss: 0.6064 - val_acc: 0.7690\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.4588 - acc: 0.8429 - val_loss: 0.6043 - val_acc: 0.7690\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4552 - acc: 0.8457 - val_loss: 0.6078 - val_acc: 0.7690\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4512 - acc: 0.8476 - val_loss: 0.6032 - val_acc: 0.7660\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4483 - acc: 0.8471 - val_loss: 0.6029 - val_acc: 0.7670\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4446 - acc: 0.8500 - val_loss: 0.6008 - val_acc: 0.7700\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.4410 - acc: 0.8508 - val_loss: 0.6004 - val_acc: 0.7660\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.4378 - acc: 0.8508 - val_loss: 0.5979 - val_acc: 0.7690\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.4340 - acc: 0.8536 - val_loss: 0.5988 - val_acc: 0.7620\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.4307 - acc: 0.8549 - val_loss: 0.5971 - val_acc: 0.7690\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4272 - acc: 0.8564 - val_loss: 0.5992 - val_acc: 0.7660\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4238 - acc: 0.8580 - val_loss: 0.5963 - val_acc: 0.7660\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4210 - acc: 0.8580 - val_loss: 0.5975 - val_acc: 0.7630\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.4174 - acc: 0.8591 - val_loss: 0.5973 - val_acc: 0.7700\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4144 - acc: 0.8601 - val_loss: 0.5942 - val_acc: 0.7740\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4112 - acc: 0.8635 - val_loss: 0.5951 - val_acc: 0.7780\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4085 - acc: 0.8640 - val_loss: 0.5919 - val_acc: 0.7710\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4052 - acc: 0.8651 - val_loss: 0.5955 - val_acc: 0.7660\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4021 - acc: 0.8673 - val_loss: 0.5921 - val_acc: 0.7710\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3985 - acc: 0.8675 - val_loss: 0.5912 - val_acc: 0.7720\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3963 - acc: 0.8687 - val_loss: 0.5940 - val_acc: 0.7740\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3933 - acc: 0.8696 - val_loss: 0.5924 - val_acc: 0.7690\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3905 - acc: 0.8705 - val_loss: 0.5924 - val_acc: 0.7690\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3872 - acc: 0.8731 - val_loss: 0.5940 - val_acc: 0.7740\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3844 - acc: 0.8736 - val_loss: 0.5885 - val_acc: 0.7700\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3820 - acc: 0.8739 - val_loss: 0.5902 - val_acc: 0.7740\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3790 - acc: 0.8756 - val_loss: 0.5903 - val_acc: 0.7730\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3765 - acc: 0.8752 - val_loss: 0.5900 - val_acc: 0.7720\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3731 - acc: 0.8759 - val_loss: 0.5892 - val_acc: 0.7730\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3709 - acc: 0.8780 - val_loss: 0.5924 - val_acc: 0.7780\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3685 - acc: 0.8787 - val_loss: 0.5907 - val_acc: 0.7690\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3656 - acc: 0.8803 - val_loss: 0.5896 - val_acc: 0.7720\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3628 - acc: 0.8803 - val_loss: 0.5915 - val_acc: 0.7750\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3601 - acc: 0.8839 - val_loss: 0.5898 - val_acc: 0.7740\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3584 - acc: 0.8815 - val_loss: 0.5887 - val_acc: 0.7720\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3551 - acc: 0.8839 - val_loss: 0.5929 - val_acc: 0.7660\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3532 - acc: 0.8855 - val_loss: 0.5917 - val_acc: 0.7830\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3509 - acc: 0.8859 - val_loss: 0.5906 - val_acc: 0.7730\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3479 - acc: 0.8869 - val_loss: 0.5964 - val_acc: 0.7780\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3457 - acc: 0.8869 - val_loss: 0.5898 - val_acc: 0.7800\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3431 - acc: 0.8881 - val_loss: 0.5904 - val_acc: 0.7710\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3407 - acc: 0.8896 - val_loss: 0.5913 - val_acc: 0.7790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.3379 - acc: 0.8903 - val_loss: 0.5898 - val_acc: 0.7770\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 27us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 40us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33441923424402875, 0.8937333333651225]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6670772852897644, 0.7786666666666666]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.9550 - acc: 0.1552 - val_loss: 1.9430 - val_acc: 0.1880\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9264 - acc: 0.1859 - val_loss: 1.9231 - val_acc: 0.2040\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9072 - acc: 0.2133 - val_loss: 1.9070 - val_acc: 0.2170\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8903 - acc: 0.2305 - val_loss: 1.8909 - val_acc: 0.2280\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.8730 - acc: 0.2400 - val_loss: 1.8738 - val_acc: 0.2430\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8541 - acc: 0.2540 - val_loss: 1.8542 - val_acc: 0.2550\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8328 - acc: 0.2617 - val_loss: 1.8321 - val_acc: 0.2700\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8089 - acc: 0.2788 - val_loss: 1.8076 - val_acc: 0.2790\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7822 - acc: 0.2948 - val_loss: 1.7798 - val_acc: 0.3000\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7522 - acc: 0.3085 - val_loss: 1.7483 - val_acc: 0.3240\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7179 - acc: 0.3292 - val_loss: 1.7128 - val_acc: 0.3590\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6793 - acc: 0.3575 - val_loss: 1.6725 - val_acc: 0.3720\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6364 - acc: 0.3755 - val_loss: 1.6284 - val_acc: 0.4000\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5905 - acc: 0.4113 - val_loss: 1.5831 - val_acc: 0.4310\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.5429 - acc: 0.4441 - val_loss: 1.5352 - val_acc: 0.4610\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4938 - acc: 0.4816 - val_loss: 1.4867 - val_acc: 0.4880\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4442 - acc: 0.5192 - val_loss: 1.4378 - val_acc: 0.5200\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3949 - acc: 0.5473 - val_loss: 1.3908 - val_acc: 0.5510\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3465 - acc: 0.5761 - val_loss: 1.3436 - val_acc: 0.5710\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2989 - acc: 0.6017 - val_loss: 1.2969 - val_acc: 0.5820\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2530 - acc: 0.6180 - val_loss: 1.2553 - val_acc: 0.6030\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2086 - acc: 0.6403 - val_loss: 1.2110 - val_acc: 0.6110\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1664 - acc: 0.6513 - val_loss: 1.1735 - val_acc: 0.6380\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1258 - acc: 0.6599 - val_loss: 1.1378 - val_acc: 0.6470\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0876 - acc: 0.6729 - val_loss: 1.0998 - val_acc: 0.6560\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0518 - acc: 0.6859 - val_loss: 1.0681 - val_acc: 0.6580\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0178 - acc: 0.6919 - val_loss: 1.0344 - val_acc: 0.6620\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9860 - acc: 0.6973 - val_loss: 1.0070 - val_acc: 0.6690\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9562 - acc: 0.7055 - val_loss: 0.9800 - val_acc: 0.6800\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9285 - acc: 0.7097 - val_loss: 0.9558 - val_acc: 0.6890\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9026 - acc: 0.7149 - val_loss: 0.9332 - val_acc: 0.6900\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8788 - acc: 0.7220 - val_loss: 0.9107 - val_acc: 0.6900\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8562 - acc: 0.7245 - val_loss: 0.8912 - val_acc: 0.6980\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8353 - acc: 0.7303 - val_loss: 0.8727 - val_acc: 0.7080\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8155 - acc: 0.7353 - val_loss: 0.8579 - val_acc: 0.6980\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7974 - acc: 0.7387 - val_loss: 0.8412 - val_acc: 0.7070\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7806 - acc: 0.7445 - val_loss: 0.8263 - val_acc: 0.7100\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7647 - acc: 0.7461 - val_loss: 0.8133 - val_acc: 0.7050\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7496 - acc: 0.7515 - val_loss: 0.8002 - val_acc: 0.7140\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7355 - acc: 0.7543 - val_loss: 0.7886 - val_acc: 0.7090\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7224 - acc: 0.7572 - val_loss: 0.7797 - val_acc: 0.7190\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7102 - acc: 0.7581 - val_loss: 0.7673 - val_acc: 0.7150\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6983 - acc: 0.7631 - val_loss: 0.7603 - val_acc: 0.7180\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6869 - acc: 0.7673 - val_loss: 0.7517 - val_acc: 0.7160\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6763 - acc: 0.7699 - val_loss: 0.7415 - val_acc: 0.7180\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6660 - acc: 0.7737 - val_loss: 0.7360 - val_acc: 0.7210\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6567 - acc: 0.7763 - val_loss: 0.7253 - val_acc: 0.7210\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6476 - acc: 0.7791 - val_loss: 0.7220 - val_acc: 0.7220\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6385 - acc: 0.7796 - val_loss: 0.7145 - val_acc: 0.7310\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6303 - acc: 0.7844 - val_loss: 0.7066 - val_acc: 0.7360\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6222 - acc: 0.7863 - val_loss: 0.7047 - val_acc: 0.7320\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6143 - acc: 0.7907 - val_loss: 0.6967 - val_acc: 0.7270\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6070 - acc: 0.7904 - val_loss: 0.6951 - val_acc: 0.7310\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5999 - acc: 0.7945 - val_loss: 0.6906 - val_acc: 0.7330\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5932 - acc: 0.7963 - val_loss: 0.6827 - val_acc: 0.7370\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5864 - acc: 0.7977 - val_loss: 0.6777 - val_acc: 0.7310\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5801 - acc: 0.8015 - val_loss: 0.6757 - val_acc: 0.7370\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5737 - acc: 0.8020 - val_loss: 0.6712 - val_acc: 0.7410\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5678 - acc: 0.8032 - val_loss: 0.6734 - val_acc: 0.7400\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5619 - acc: 0.8072 - val_loss: 0.6657 - val_acc: 0.7400\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 24us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5579499176979065, 0.8077333333015442]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7057990253766377, 0.7413333333333333]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.5909 - acc: 0.1928 - val_loss: 2.5822 - val_acc: 0.2040\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.5702 - acc: 0.2063 - val_loss: 2.5645 - val_acc: 0.2140\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5518 - acc: 0.2208 - val_loss: 2.5476 - val_acc: 0.2300\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5330 - acc: 0.2335 - val_loss: 2.5295 - val_acc: 0.2440\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.5122 - acc: 0.2505 - val_loss: 2.5093 - val_acc: 0.2640\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.4891 - acc: 0.2731 - val_loss: 2.4861 - val_acc: 0.2890\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.4631 - acc: 0.2957 - val_loss: 2.4595 - val_acc: 0.3130\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.4338 - acc: 0.3180 - val_loss: 2.4284 - val_acc: 0.3370\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.4003 - acc: 0.3441 - val_loss: 2.3936 - val_acc: 0.3620\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.3626 - acc: 0.3727 - val_loss: 2.3562 - val_acc: 0.3890\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.3222 - acc: 0.4029 - val_loss: 2.3157 - val_acc: 0.4240\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.2786 - acc: 0.4360 - val_loss: 2.2720 - val_acc: 0.4370\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.2329 - acc: 0.4583 - val_loss: 2.2267 - val_acc: 0.4630\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.1849 - acc: 0.4935 - val_loss: 2.1806 - val_acc: 0.4930\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.1357 - acc: 0.5196 - val_loss: 2.1324 - val_acc: 0.5260\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.0850 - acc: 0.5447 - val_loss: 2.0810 - val_acc: 0.5440\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.0336 - acc: 0.5713 - val_loss: 2.0321 - val_acc: 0.5800\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.9823 - acc: 0.5960 - val_loss: 1.9826 - val_acc: 0.5970\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9314 - acc: 0.6136 - val_loss: 1.9317 - val_acc: 0.6100\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8811 - acc: 0.6356 - val_loss: 1.8864 - val_acc: 0.6300\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8326 - acc: 0.6493 - val_loss: 1.8395 - val_acc: 0.6370\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7852 - acc: 0.6663 - val_loss: 1.7958 - val_acc: 0.6580\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7403 - acc: 0.6785 - val_loss: 1.7512 - val_acc: 0.6760\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6975 - acc: 0.6911 - val_loss: 1.7122 - val_acc: 0.6840\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6563 - acc: 0.7011 - val_loss: 1.6739 - val_acc: 0.6920\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6180 - acc: 0.7089 - val_loss: 1.6382 - val_acc: 0.6960\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5820 - acc: 0.7135 - val_loss: 1.6058 - val_acc: 0.7010\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5489 - acc: 0.7205 - val_loss: 1.5783 - val_acc: 0.7030\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5177 - acc: 0.7273 - val_loss: 1.5491 - val_acc: 0.7070\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4886 - acc: 0.7293 - val_loss: 1.5230 - val_acc: 0.7140\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4617 - acc: 0.7361 - val_loss: 1.4989 - val_acc: 0.7100\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4363 - acc: 0.7423 - val_loss: 1.4791 - val_acc: 0.7160\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4139 - acc: 0.7448 - val_loss: 1.4569 - val_acc: 0.7150\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3921 - acc: 0.7493 - val_loss: 1.4404 - val_acc: 0.7180\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3724 - acc: 0.7545 - val_loss: 1.4222 - val_acc: 0.7250\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3538 - acc: 0.7557 - val_loss: 1.4043 - val_acc: 0.7280\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3361 - acc: 0.7599 - val_loss: 1.3928 - val_acc: 0.7250\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3200 - acc: 0.7644 - val_loss: 1.3770 - val_acc: 0.7230\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3042 - acc: 0.7688 - val_loss: 1.3634 - val_acc: 0.7250\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2898 - acc: 0.7719 - val_loss: 1.3535 - val_acc: 0.7290\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2763 - acc: 0.7728 - val_loss: 1.3418 - val_acc: 0.7290\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2632 - acc: 0.7777 - val_loss: 1.3313 - val_acc: 0.7310\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2509 - acc: 0.7801 - val_loss: 1.3226 - val_acc: 0.7310\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2393 - acc: 0.7804 - val_loss: 1.3118 - val_acc: 0.7400\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2281 - acc: 0.7844 - val_loss: 1.3050 - val_acc: 0.7360\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2173 - acc: 0.7895 - val_loss: 1.2953 - val_acc: 0.7440\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2070 - acc: 0.7896 - val_loss: 1.2868 - val_acc: 0.7420\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.1969 - acc: 0.7937 - val_loss: 1.2806 - val_acc: 0.7390\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1878 - acc: 0.7959 - val_loss: 1.2743 - val_acc: 0.7460\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1782 - acc: 0.7971 - val_loss: 1.2659 - val_acc: 0.7490\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1693 - acc: 0.8012 - val_loss: 1.2602 - val_acc: 0.7490\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1607 - acc: 0.8019 - val_loss: 1.2536 - val_acc: 0.7510\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1525 - acc: 0.8036 - val_loss: 1.2474 - val_acc: 0.7460\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1446 - acc: 0.8072 - val_loss: 1.2420 - val_acc: 0.7500\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1362 - acc: 0.8083 - val_loss: 1.2370 - val_acc: 0.7500\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1285 - acc: 0.8103 - val_loss: 1.2303 - val_acc: 0.7510\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1210 - acc: 0.8117 - val_loss: 1.2243 - val_acc: 0.7450\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1140 - acc: 0.8152 - val_loss: 1.2186 - val_acc: 0.7460\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1065 - acc: 0.8165 - val_loss: 1.2142 - val_acc: 0.7500\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0996 - acc: 0.8199 - val_loss: 1.2099 - val_acc: 0.7470\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0929 - acc: 0.8195 - val_loss: 1.2055 - val_acc: 0.7460\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0862 - acc: 0.8209 - val_loss: 1.2005 - val_acc: 0.7510\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0793 - acc: 0.8247 - val_loss: 1.1972 - val_acc: 0.7510\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0734 - acc: 0.8259 - val_loss: 1.1906 - val_acc: 0.7530\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0665 - acc: 0.8277 - val_loss: 1.1869 - val_acc: 0.7550\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0603 - acc: 0.8291 - val_loss: 1.1826 - val_acc: 0.7530\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0542 - acc: 0.8304 - val_loss: 1.1775 - val_acc: 0.7550\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0480 - acc: 0.8321 - val_loss: 1.1754 - val_acc: 0.7550\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0419 - acc: 0.8333 - val_loss: 1.1721 - val_acc: 0.7560\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0359 - acc: 0.8347 - val_loss: 1.1678 - val_acc: 0.7570\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0299 - acc: 0.8364 - val_loss: 1.1631 - val_acc: 0.7590\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0239 - acc: 0.8373 - val_loss: 1.1591 - val_acc: 0.7530\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0184 - acc: 0.8392 - val_loss: 1.1576 - val_acc: 0.7590\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0130 - acc: 0.8419 - val_loss: 1.1522 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0080 - acc: 0.8427 - val_loss: 1.1491 - val_acc: 0.7600\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0020 - acc: 0.8423 - val_loss: 1.1465 - val_acc: 0.7560\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9970 - acc: 0.8464 - val_loss: 1.1432 - val_acc: 0.7570\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9918 - acc: 0.8460 - val_loss: 1.1403 - val_acc: 0.7560\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9865 - acc: 0.8491 - val_loss: 1.1367 - val_acc: 0.7610\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9814 - acc: 0.8492 - val_loss: 1.1324 - val_acc: 0.7640\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9765 - acc: 0.8505 - val_loss: 1.1291 - val_acc: 0.7650\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9718 - acc: 0.8521 - val_loss: 1.1294 - val_acc: 0.7600\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9666 - acc: 0.8537 - val_loss: 1.1284 - val_acc: 0.7660\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9622 - acc: 0.8537 - val_loss: 1.1211 - val_acc: 0.7620\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9574 - acc: 0.8540 - val_loss: 1.1201 - val_acc: 0.7610\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9527 - acc: 0.8563 - val_loss: 1.1161 - val_acc: 0.7610\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9484 - acc: 0.8561 - val_loss: 1.1130 - val_acc: 0.7670\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9434 - acc: 0.8595 - val_loss: 1.1112 - val_acc: 0.7620\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9391 - acc: 0.8584 - val_loss: 1.1082 - val_acc: 0.7650\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9343 - acc: 0.8596 - val_loss: 1.1033 - val_acc: 0.7660\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9300 - acc: 0.8617 - val_loss: 1.1025 - val_acc: 0.7630\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9256 - acc: 0.8639 - val_loss: 1.1003 - val_acc: 0.7670\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9216 - acc: 0.8631 - val_loss: 1.0964 - val_acc: 0.7660\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9173 - acc: 0.8661 - val_loss: 1.0970 - val_acc: 0.7650\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9130 - acc: 0.8657 - val_loss: 1.0920 - val_acc: 0.7680\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.9090 - acc: 0.8661 - val_loss: 1.0892 - val_acc: 0.7660\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9045 - acc: 0.8677 - val_loss: 1.0900 - val_acc: 0.7680\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9012 - acc: 0.8675 - val_loss: 1.0849 - val_acc: 0.7690\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8965 - acc: 0.8699 - val_loss: 1.0809 - val_acc: 0.7660\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8922 - acc: 0.8712 - val_loss: 1.0823 - val_acc: 0.7710\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8885 - acc: 0.8717 - val_loss: 1.0778 - val_acc: 0.7650\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8848 - acc: 0.8736 - val_loss: 1.0773 - val_acc: 0.7690\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8810 - acc: 0.8735 - val_loss: 1.0737 - val_acc: 0.7670\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8769 - acc: 0.8752 - val_loss: 1.0722 - val_acc: 0.7700\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8732 - acc: 0.8763 - val_loss: 1.0710 - val_acc: 0.7740\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8693 - acc: 0.8785 - val_loss: 1.0740 - val_acc: 0.7670\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8660 - acc: 0.8773 - val_loss: 1.0650 - val_acc: 0.7650\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8616 - acc: 0.8785 - val_loss: 1.0630 - val_acc: 0.7700\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8581 - acc: 0.8807 - val_loss: 1.0606 - val_acc: 0.7720\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8545 - acc: 0.8791 - val_loss: 1.0594 - val_acc: 0.7700\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8511 - acc: 0.8813 - val_loss: 1.0613 - val_acc: 0.7690\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8475 - acc: 0.8827 - val_loss: 1.0576 - val_acc: 0.7710\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8442 - acc: 0.8813 - val_loss: 1.0548 - val_acc: 0.7700\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8407 - acc: 0.8832 - val_loss: 1.0512 - val_acc: 0.7720\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8369 - acc: 0.8837 - val_loss: 1.0487 - val_acc: 0.7720\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8333 - acc: 0.8859 - val_loss: 1.0488 - val_acc: 0.7720\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8302 - acc: 0.8860 - val_loss: 1.0462 - val_acc: 0.7720\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8266 - acc: 0.8871 - val_loss: 1.0440 - val_acc: 0.7750\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8234 - acc: 0.8875 - val_loss: 1.0433 - val_acc: 0.7700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8201 - acc: 0.8896 - val_loss: 1.0416 - val_acc: 0.7730\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 15.9945 - acc: 0.1688 - val_loss: 15.5886 - val_acc: 0.1830\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 15.2419 - acc: 0.1843 - val_loss: 14.8494 - val_acc: 0.2070\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 14.5117 - acc: 0.2008 - val_loss: 14.1312 - val_acc: 0.2140\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 13.8029 - acc: 0.2095 - val_loss: 13.4339 - val_acc: 0.2320\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 13.1143 - acc: 0.2180 - val_loss: 12.7562 - val_acc: 0.2360\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 12.4455 - acc: 0.2261 - val_loss: 12.0991 - val_acc: 0.2430\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 11.7971 - acc: 0.2400 - val_loss: 11.4619 - val_acc: 0.2570\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 11.1685 - acc: 0.2533 - val_loss: 10.8448 - val_acc: 0.2650\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 10.5599 - acc: 0.2625 - val_loss: 10.2479 - val_acc: 0.2830\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 9.9708 - acc: 0.2792 - val_loss: 9.6708 - val_acc: 0.3030\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 9.4024 - acc: 0.2972 - val_loss: 9.1139 - val_acc: 0.3210\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 8.8554 - acc: 0.3171 - val_loss: 8.5785 - val_acc: 0.3410\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 8.3301 - acc: 0.3409 - val_loss: 8.0644 - val_acc: 0.3570\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 7.8261 - acc: 0.3580 - val_loss: 7.5717 - val_acc: 0.3560\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 7.3430 - acc: 0.3716 - val_loss: 7.0995 - val_acc: 0.3900\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 6.8814 - acc: 0.3901 - val_loss: 6.6498 - val_acc: 0.4080\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 6.4415 - acc: 0.4041 - val_loss: 6.2204 - val_acc: 0.4090\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 6.0239 - acc: 0.4067 - val_loss: 5.8152 - val_acc: 0.4110\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 5.6288 - acc: 0.4155 - val_loss: 5.4311 - val_acc: 0.4210\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 5.2561 - acc: 0.4224 - val_loss: 5.0706 - val_acc: 0.4290\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 4.9055 - acc: 0.4281 - val_loss: 4.7311 - val_acc: 0.4460\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 4.5770 - acc: 0.4393 - val_loss: 4.4149 - val_acc: 0.4480\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 4.2708 - acc: 0.4485 - val_loss: 4.1219 - val_acc: 0.4630\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 3.9862 - acc: 0.4567 - val_loss: 3.8497 - val_acc: 0.4820\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 3.7235 - acc: 0.4728 - val_loss: 3.5962 - val_acc: 0.4730\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 3.4823 - acc: 0.4840 - val_loss: 3.3672 - val_acc: 0.4870\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.2630 - acc: 0.4955 - val_loss: 3.1592 - val_acc: 0.5100\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 3.0657 - acc: 0.5133 - val_loss: 2.9737 - val_acc: 0.5040\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.8899 - acc: 0.5189 - val_loss: 2.8097 - val_acc: 0.5150\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.7349 - acc: 0.5292 - val_loss: 2.6652 - val_acc: 0.5290\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.6000 - acc: 0.5431 - val_loss: 2.5429 - val_acc: 0.5440\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.4856 - acc: 0.5523 - val_loss: 2.4375 - val_acc: 0.5450\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.3908 - acc: 0.5527 - val_loss: 2.3521 - val_acc: 0.5500\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.3150 - acc: 0.5601 - val_loss: 2.2876 - val_acc: 0.5570\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.2564 - acc: 0.5649 - val_loss: 2.2374 - val_acc: 0.5560\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.2137 - acc: 0.5681 - val_loss: 2.2012 - val_acc: 0.5600\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.1828 - acc: 0.5685 - val_loss: 2.1739 - val_acc: 0.5560\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.1579 - acc: 0.5713 - val_loss: 2.1513 - val_acc: 0.5700\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.1351 - acc: 0.5764 - val_loss: 2.1286 - val_acc: 0.5680\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.1141 - acc: 0.5769 - val_loss: 2.1142 - val_acc: 0.5820\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0938 - acc: 0.5797 - val_loss: 2.0906 - val_acc: 0.5820\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0744 - acc: 0.5820 - val_loss: 2.0701 - val_acc: 0.5790\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0556 - acc: 0.5872 - val_loss: 2.0512 - val_acc: 0.5810\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0377 - acc: 0.5865 - val_loss: 2.0336 - val_acc: 0.5890\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0198 - acc: 0.5896 - val_loss: 2.0176 - val_acc: 0.5920\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0027 - acc: 0.5895 - val_loss: 2.0005 - val_acc: 0.5910\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9855 - acc: 0.5931 - val_loss: 1.9835 - val_acc: 0.6020\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9688 - acc: 0.5969 - val_loss: 1.9680 - val_acc: 0.6070\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9524 - acc: 0.6005 - val_loss: 1.9501 - val_acc: 0.6030\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9366 - acc: 0.6008 - val_loss: 1.9347 - val_acc: 0.6080\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9210 - acc: 0.6075 - val_loss: 1.9186 - val_acc: 0.6090\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9055 - acc: 0.6088 - val_loss: 1.9048 - val_acc: 0.6190\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8907 - acc: 0.6144 - val_loss: 1.8892 - val_acc: 0.6230\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8759 - acc: 0.6168 - val_loss: 1.8739 - val_acc: 0.6290\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8615 - acc: 0.6236 - val_loss: 1.8592 - val_acc: 0.6290\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8477 - acc: 0.6287 - val_loss: 1.8476 - val_acc: 0.6370\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8341 - acc: 0.6307 - val_loss: 1.8319 - val_acc: 0.6420\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8214 - acc: 0.6349 - val_loss: 1.8189 - val_acc: 0.6410\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8081 - acc: 0.6384 - val_loss: 1.8074 - val_acc: 0.6510\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7957 - acc: 0.6413 - val_loss: 1.7955 - val_acc: 0.6550\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7834 - acc: 0.6444 - val_loss: 1.7839 - val_acc: 0.6500\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7713 - acc: 0.6432 - val_loss: 1.7720 - val_acc: 0.6640\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7598 - acc: 0.6479 - val_loss: 1.7623 - val_acc: 0.6530\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7486 - acc: 0.6508 - val_loss: 1.7477 - val_acc: 0.6640\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7371 - acc: 0.6536 - val_loss: 1.7382 - val_acc: 0.6680\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7262 - acc: 0.6535 - val_loss: 1.7278 - val_acc: 0.6650\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7159 - acc: 0.6545 - val_loss: 1.7150 - val_acc: 0.6730\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7053 - acc: 0.6576 - val_loss: 1.7059 - val_acc: 0.6680\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6951 - acc: 0.6599 - val_loss: 1.6962 - val_acc: 0.6740\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6849 - acc: 0.6605 - val_loss: 1.6858 - val_acc: 0.6740\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6747 - acc: 0.6628 - val_loss: 1.6776 - val_acc: 0.6760\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6655 - acc: 0.6665 - val_loss: 1.6638 - val_acc: 0.6770\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6554 - acc: 0.6665 - val_loss: 1.6551 - val_acc: 0.6770\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6460 - acc: 0.6660 - val_loss: 1.6473 - val_acc: 0.6780\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6374 - acc: 0.6711 - val_loss: 1.6360 - val_acc: 0.6780\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6284 - acc: 0.6716 - val_loss: 1.6282 - val_acc: 0.6810\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6191 - acc: 0.6719 - val_loss: 1.6202 - val_acc: 0.6880\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6106 - acc: 0.6727 - val_loss: 1.6101 - val_acc: 0.6840\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6019 - acc: 0.6724 - val_loss: 1.6014 - val_acc: 0.6910\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5934 - acc: 0.6752 - val_loss: 1.5941 - val_acc: 0.6850\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5855 - acc: 0.6747 - val_loss: 1.5897 - val_acc: 0.6870\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5772 - acc: 0.6771 - val_loss: 1.5774 - val_acc: 0.6940\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5691 - acc: 0.6779 - val_loss: 1.5718 - val_acc: 0.6900\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5608 - acc: 0.6795 - val_loss: 1.5607 - val_acc: 0.6910\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5528 - acc: 0.6781 - val_loss: 1.5528 - val_acc: 0.6900\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5456 - acc: 0.6795 - val_loss: 1.5465 - val_acc: 0.6990\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5380 - acc: 0.6813 - val_loss: 1.5385 - val_acc: 0.6960\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5303 - acc: 0.6824 - val_loss: 1.5293 - val_acc: 0.7000\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5230 - acc: 0.6851 - val_loss: 1.5235 - val_acc: 0.7050\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5155 - acc: 0.6837 - val_loss: 1.5156 - val_acc: 0.7000\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5079 - acc: 0.6863 - val_loss: 1.5070 - val_acc: 0.6980\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5011 - acc: 0.6855 - val_loss: 1.5028 - val_acc: 0.6960\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4937 - acc: 0.6851 - val_loss: 1.4938 - val_acc: 0.7010\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4868 - acc: 0.6883 - val_loss: 1.4859 - val_acc: 0.7050\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4795 - acc: 0.6896 - val_loss: 1.4790 - val_acc: 0.7070\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4725 - acc: 0.6903 - val_loss: 1.4713 - val_acc: 0.7090\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4659 - acc: 0.6900 - val_loss: 1.4628 - val_acc: 0.7090\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4591 - acc: 0.6904 - val_loss: 1.4576 - val_acc: 0.7080\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4526 - acc: 0.6923 - val_loss: 1.4523 - val_acc: 0.7100\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4468 - acc: 0.6928 - val_loss: 1.4442 - val_acc: 0.7110\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4398 - acc: 0.6937 - val_loss: 1.4367 - val_acc: 0.7110\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4328 - acc: 0.6929 - val_loss: 1.4305 - val_acc: 0.7160\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4263 - acc: 0.6940 - val_loss: 1.4240 - val_acc: 0.7170\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.4203 - acc: 0.6960 - val_loss: 1.4198 - val_acc: 0.7150\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4143 - acc: 0.6960 - val_loss: 1.4123 - val_acc: 0.7170\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4081 - acc: 0.6980 - val_loss: 1.4055 - val_acc: 0.7170\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4019 - acc: 0.6995 - val_loss: 1.3985 - val_acc: 0.7140\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3955 - acc: 0.7001 - val_loss: 1.3931 - val_acc: 0.7170\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3897 - acc: 0.6993 - val_loss: 1.3893 - val_acc: 0.7180\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3841 - acc: 0.7009 - val_loss: 1.3827 - val_acc: 0.7170\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3780 - acc: 0.7019 - val_loss: 1.3754 - val_acc: 0.7200\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3719 - acc: 0.7021 - val_loss: 1.3693 - val_acc: 0.7200\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3657 - acc: 0.7019 - val_loss: 1.3658 - val_acc: 0.7230\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3606 - acc: 0.7043 - val_loss: 1.3579 - val_acc: 0.7200\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3544 - acc: 0.7039 - val_loss: 1.3557 - val_acc: 0.7200\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3492 - acc: 0.7044 - val_loss: 1.3491 - val_acc: 0.7240\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3435 - acc: 0.7029 - val_loss: 1.3400 - val_acc: 0.7220\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3377 - acc: 0.7049 - val_loss: 1.3361 - val_acc: 0.7200\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3321 - acc: 0.7075 - val_loss: 1.3313 - val_acc: 0.7210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3274 - acc: 0.7065 - val_loss: 1.3258 - val_acc: 0.7240\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 15.9783 - acc: 0.1525 - val_loss: 15.5623 - val_acc: 0.1730\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 15.2122 - acc: 0.1859 - val_loss: 14.8160 - val_acc: 0.2110\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 14.4768 - acc: 0.2061 - val_loss: 14.0932 - val_acc: 0.2330\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 13.7636 - acc: 0.2283 - val_loss: 13.3920 - val_acc: 0.2510\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 13.0716 - acc: 0.2515 - val_loss: 12.7119 - val_acc: 0.2740\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 12.4011 - acc: 0.2743 - val_loss: 12.0517 - val_acc: 0.3010\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 11.7510 - acc: 0.2915 - val_loss: 11.4126 - val_acc: 0.3200\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 11.1216 - acc: 0.3163 - val_loss: 10.7918 - val_acc: 0.3510\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 10.5109 - acc: 0.3365 - val_loss: 10.1896 - val_acc: 0.3710\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 9.9191 - acc: 0.3551 - val_loss: 9.6081 - val_acc: 0.3870\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 9.3480 - acc: 0.3757 - val_loss: 9.0472 - val_acc: 0.4030\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 8.7970 - acc: 0.3955 - val_loss: 8.5062 - val_acc: 0.4180\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 8.2665 - acc: 0.4145 - val_loss: 7.9872 - val_acc: 0.4240\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 7.7573 - acc: 0.4285 - val_loss: 7.4885 - val_acc: 0.4570\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 7.2692 - acc: 0.4467 - val_loss: 7.0132 - val_acc: 0.4710\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 6.8034 - acc: 0.4719 - val_loss: 6.5588 - val_acc: 0.4810\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 6.3599 - acc: 0.4872 - val_loss: 6.1270 - val_acc: 0.5220\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 5.9392 - acc: 0.5035 - val_loss: 5.7183 - val_acc: 0.5360\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 5.5410 - acc: 0.5201 - val_loss: 5.3321 - val_acc: 0.5620\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 5.1656 - acc: 0.5423 - val_loss: 4.9691 - val_acc: 0.5670\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 4.8124 - acc: 0.5617 - val_loss: 4.6275 - val_acc: 0.5770\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 4.4821 - acc: 0.5743 - val_loss: 4.3103 - val_acc: 0.5810\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 4.1745 - acc: 0.5883 - val_loss: 4.0147 - val_acc: 0.6020\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 3.8890 - acc: 0.5988 - val_loss: 3.7427 - val_acc: 0.6120\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 3.6265 - acc: 0.6112 - val_loss: 3.4922 - val_acc: 0.6360\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 3.3859 - acc: 0.6269 - val_loss: 3.2633 - val_acc: 0.6440\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 3.1666 - acc: 0.6301 - val_loss: 3.0551 - val_acc: 0.6340\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.9690 - acc: 0.6385 - val_loss: 2.8694 - val_acc: 0.6410\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.7935 - acc: 0.6425 - val_loss: 2.7045 - val_acc: 0.6330\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.6386 - acc: 0.6464 - val_loss: 2.5616 - val_acc: 0.6310\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.5038 - acc: 0.6476 - val_loss: 2.4362 - val_acc: 0.6350\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.3891 - acc: 0.6501 - val_loss: 2.3314 - val_acc: 0.6560\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.2932 - acc: 0.6525 - val_loss: 2.2462 - val_acc: 0.6550\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.2161 - acc: 0.6540 - val_loss: 2.1764 - val_acc: 0.6670\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.1563 - acc: 0.6568 - val_loss: 2.1288 - val_acc: 0.6570\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.1120 - acc: 0.6579 - val_loss: 2.0875 - val_acc: 0.6660\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.0786 - acc: 0.6567 - val_loss: 2.0597 - val_acc: 0.6640\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 2.0517 - acc: 0.6587 - val_loss: 2.0329 - val_acc: 0.6640\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.0276 - acc: 0.6620 - val_loss: 2.0097 - val_acc: 0.6710\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0051 - acc: 0.6637 - val_loss: 1.9901 - val_acc: 0.6670\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9845 - acc: 0.6648 - val_loss: 1.9687 - val_acc: 0.6760\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9649 - acc: 0.6663 - val_loss: 1.9504 - val_acc: 0.6750\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9463 - acc: 0.6648 - val_loss: 1.9323 - val_acc: 0.6650\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9289 - acc: 0.6652 - val_loss: 1.9173 - val_acc: 0.6730\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9122 - acc: 0.6663 - val_loss: 1.8987 - val_acc: 0.6710\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8956 - acc: 0.6679 - val_loss: 1.8810 - val_acc: 0.6740\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8797 - acc: 0.6664 - val_loss: 1.8651 - val_acc: 0.6850\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8644 - acc: 0.6697 - val_loss: 1.8543 - val_acc: 0.6870\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8506 - acc: 0.6724 - val_loss: 1.8350 - val_acc: 0.6850\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8359 - acc: 0.6724 - val_loss: 1.8251 - val_acc: 0.6800\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8227 - acc: 0.6729 - val_loss: 1.8104 - val_acc: 0.6820\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8095 - acc: 0.6733 - val_loss: 1.7985 - val_acc: 0.6870\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7972 - acc: 0.6735 - val_loss: 1.7893 - val_acc: 0.6870\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7842 - acc: 0.6768 - val_loss: 1.7734 - val_acc: 0.6850\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7723 - acc: 0.6745 - val_loss: 1.7649 - val_acc: 0.6800\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7608 - acc: 0.6760 - val_loss: 1.7468 - val_acc: 0.6850\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7491 - acc: 0.6764 - val_loss: 1.7358 - val_acc: 0.6880\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7380 - acc: 0.6769 - val_loss: 1.7281 - val_acc: 0.6750\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7273 - acc: 0.6773 - val_loss: 1.7151 - val_acc: 0.6860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7167 - acc: 0.6803 - val_loss: 1.7063 - val_acc: 0.6880\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7067 - acc: 0.6800 - val_loss: 1.6930 - val_acc: 0.6870\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6965 - acc: 0.6796 - val_loss: 1.6902 - val_acc: 0.6910\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6875 - acc: 0.6804 - val_loss: 1.6745 - val_acc: 0.6970\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6770 - acc: 0.6808 - val_loss: 1.6644 - val_acc: 0.6890\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6680 - acc: 0.6825 - val_loss: 1.6554 - val_acc: 0.6840\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6586 - acc: 0.6828 - val_loss: 1.6466 - val_acc: 0.6930\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6492 - acc: 0.6828 - val_loss: 1.6394 - val_acc: 0.6960\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6410 - acc: 0.6828 - val_loss: 1.6289 - val_acc: 0.6940\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6322 - acc: 0.6851 - val_loss: 1.6199 - val_acc: 0.6940\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6232 - acc: 0.6861 - val_loss: 1.6144 - val_acc: 0.6900\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6152 - acc: 0.6855 - val_loss: 1.6074 - val_acc: 0.6860\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6070 - acc: 0.6869 - val_loss: 1.5942 - val_acc: 0.6930\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5987 - acc: 0.6881 - val_loss: 1.5858 - val_acc: 0.6930\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5901 - acc: 0.6896 - val_loss: 1.5774 - val_acc: 0.6920\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5827 - acc: 0.6895 - val_loss: 1.5711 - val_acc: 0.7010\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5751 - acc: 0.6908 - val_loss: 1.5658 - val_acc: 0.7000\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5677 - acc: 0.6921 - val_loss: 1.5543 - val_acc: 0.6980\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5594 - acc: 0.6904 - val_loss: 1.5533 - val_acc: 0.7010\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5523 - acc: 0.6923 - val_loss: 1.5405 - val_acc: 0.7010\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5448 - acc: 0.6911 - val_loss: 1.5333 - val_acc: 0.7000\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5378 - acc: 0.6940 - val_loss: 1.5315 - val_acc: 0.7010\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5306 - acc: 0.6935 - val_loss: 1.5185 - val_acc: 0.6970\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5232 - acc: 0.6931 - val_loss: 1.5103 - val_acc: 0.7000\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5161 - acc: 0.6948 - val_loss: 1.5067 - val_acc: 0.7060\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5096 - acc: 0.6943 - val_loss: 1.5040 - val_acc: 0.6970\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.5024 - acc: 0.6960 - val_loss: 1.4920 - val_acc: 0.7010\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4957 - acc: 0.6961 - val_loss: 1.4853 - val_acc: 0.6990\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4896 - acc: 0.6960 - val_loss: 1.4789 - val_acc: 0.7050\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4826 - acc: 0.6981 - val_loss: 1.4697 - val_acc: 0.7010\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4760 - acc: 0.6980 - val_loss: 1.4646 - val_acc: 0.7030\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4694 - acc: 0.6984 - val_loss: 1.4577 - val_acc: 0.7040\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4627 - acc: 0.6997 - val_loss: 1.4525 - val_acc: 0.7020\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4572 - acc: 0.7003 - val_loss: 1.4499 - val_acc: 0.7010\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4505 - acc: 0.6995 - val_loss: 1.4386 - val_acc: 0.7000\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4442 - acc: 0.6996 - val_loss: 1.4364 - val_acc: 0.7010\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4384 - acc: 0.7003 - val_loss: 1.4267 - val_acc: 0.7040\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4321 - acc: 0.7021 - val_loss: 1.4231 - val_acc: 0.7010\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4260 - acc: 0.7023 - val_loss: 1.4187 - val_acc: 0.7090\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4204 - acc: 0.7036 - val_loss: 1.4097 - val_acc: 0.7040\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4144 - acc: 0.7021 - val_loss: 1.4062 - val_acc: 0.7040\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4085 - acc: 0.7033 - val_loss: 1.3995 - val_acc: 0.7040\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4025 - acc: 0.7031 - val_loss: 1.3911 - val_acc: 0.7060\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3965 - acc: 0.7033 - val_loss: 1.3867 - val_acc: 0.7060\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3916 - acc: 0.7053 - val_loss: 1.3810 - val_acc: 0.7090\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3857 - acc: 0.7044 - val_loss: 1.3741 - val_acc: 0.7100\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3798 - acc: 0.7056 - val_loss: 1.3700 - val_acc: 0.7090\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3743 - acc: 0.7036 - val_loss: 1.3640 - val_acc: 0.7090\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3693 - acc: 0.7056 - val_loss: 1.3595 - val_acc: 0.7120\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3641 - acc: 0.7055 - val_loss: 1.3535 - val_acc: 0.7100\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3582 - acc: 0.7056 - val_loss: 1.3611 - val_acc: 0.6990\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3541 - acc: 0.7076 - val_loss: 1.3469 - val_acc: 0.7130\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3476 - acc: 0.7080 - val_loss: 1.3396 - val_acc: 0.7060\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3423 - acc: 0.7079 - val_loss: 1.3335 - val_acc: 0.7080\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3376 - acc: 0.7087 - val_loss: 1.3264 - val_acc: 0.7120\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3320 - acc: 0.7084 - val_loss: 1.3235 - val_acc: 0.7140\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3272 - acc: 0.7095 - val_loss: 1.3192 - val_acc: 0.7140\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3226 - acc: 0.7091 - val_loss: 1.3184 - val_acc: 0.7140\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3173 - acc: 0.7085 - val_loss: 1.3097 - val_acc: 0.7120\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3122 - acc: 0.7108 - val_loss: 1.3061 - val_acc: 0.7140\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3073 - acc: 0.7109 - val_loss: 1.2991 - val_acc: 0.7120\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3022 - acc: 0.7123 - val_loss: 1.2913 - val_acc: 0.7130\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2980 - acc: 0.7120 - val_loss: 1.2902 - val_acc: 0.7150\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2937 - acc: 0.7119 - val_loss: 1.2868 - val_acc: 0.7160\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2885 - acc: 0.7104 - val_loss: 1.2824 - val_acc: 0.7110\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2837 - acc: 0.7120 - val_loss: 1.2761 - val_acc: 0.7160\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2791 - acc: 0.7125 - val_loss: 1.2722 - val_acc: 0.7140\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2750 - acc: 0.7139 - val_loss: 1.2657 - val_acc: 0.7140\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2698 - acc: 0.7137 - val_loss: 1.2620 - val_acc: 0.7120\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2655 - acc: 0.7147 - val_loss: 1.2571 - val_acc: 0.7150\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2611 - acc: 0.7147 - val_loss: 1.2517 - val_acc: 0.7100\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2574 - acc: 0.7132 - val_loss: 1.2510 - val_acc: 0.7150\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2528 - acc: 0.7159 - val_loss: 1.2452 - val_acc: 0.7130\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2490 - acc: 0.7161 - val_loss: 1.2447 - val_acc: 0.7120\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2448 - acc: 0.7165 - val_loss: 1.2458 - val_acc: 0.7110\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2408 - acc: 0.7168 - val_loss: 1.2341 - val_acc: 0.7170\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2361 - acc: 0.7153 - val_loss: 1.2350 - val_acc: 0.7150\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2322 - acc: 0.7168 - val_loss: 1.2249 - val_acc: 0.7140\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2288 - acc: 0.7165 - val_loss: 1.2240 - val_acc: 0.7190\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2249 - acc: 0.7169 - val_loss: 1.2226 - val_acc: 0.7200\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2209 - acc: 0.7169 - val_loss: 1.2143 - val_acc: 0.7170\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2172 - acc: 0.7177 - val_loss: 1.2127 - val_acc: 0.7190\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2140 - acc: 0.7181 - val_loss: 1.2068 - val_acc: 0.7140\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2099 - acc: 0.7172 - val_loss: 1.2072 - val_acc: 0.7170\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2060 - acc: 0.7187 - val_loss: 1.1998 - val_acc: 0.7180\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2029 - acc: 0.7176 - val_loss: 1.1995 - val_acc: 0.7180\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1994 - acc: 0.7191 - val_loss: 1.1948 - val_acc: 0.7190\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1959 - acc: 0.7208 - val_loss: 1.1934 - val_acc: 0.7160\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1932 - acc: 0.7203 - val_loss: 1.1891 - val_acc: 0.7230\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1894 - acc: 0.7217 - val_loss: 1.1879 - val_acc: 0.7190\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1863 - acc: 0.7212 - val_loss: 1.1840 - val_acc: 0.7180\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1827 - acc: 0.7209 - val_loss: 1.1767 - val_acc: 0.7180\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1794 - acc: 0.7201 - val_loss: 1.1784 - val_acc: 0.7190\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1763 - acc: 0.7211 - val_loss: 1.1763 - val_acc: 0.7210\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1730 - acc: 0.7213 - val_loss: 1.1700 - val_acc: 0.7180\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1705 - acc: 0.7224 - val_loss: 1.1661 - val_acc: 0.7210\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1663 - acc: 0.7239 - val_loss: 1.1626 - val_acc: 0.7200\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1643 - acc: 0.7220 - val_loss: 1.1595 - val_acc: 0.7190\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1614 - acc: 0.7225 - val_loss: 1.1711 - val_acc: 0.7200\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1583 - acc: 0.7255 - val_loss: 1.1541 - val_acc: 0.7200\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1557 - acc: 0.7248 - val_loss: 1.1593 - val_acc: 0.7160\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1526 - acc: 0.7243 - val_loss: 1.1545 - val_acc: 0.7220\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1501 - acc: 0.7251 - val_loss: 1.1487 - val_acc: 0.7230\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1465 - acc: 0.7236 - val_loss: 1.1474 - val_acc: 0.7230\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1445 - acc: 0.7263 - val_loss: 1.1406 - val_acc: 0.7220\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1409 - acc: 0.7260 - val_loss: 1.1428 - val_acc: 0.7240\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1392 - acc: 0.7257 - val_loss: 1.1372 - val_acc: 0.7210\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1376 - acc: 0.7267 - val_loss: 1.1351 - val_acc: 0.7250\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1341 - acc: 0.7269 - val_loss: 1.1351 - val_acc: 0.7230\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1315 - acc: 0.7288 - val_loss: 1.1316 - val_acc: 0.7270\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1298 - acc: 0.7257 - val_loss: 1.1329 - val_acc: 0.7260\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1270 - acc: 0.7297 - val_loss: 1.1246 - val_acc: 0.7240\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1244 - acc: 0.7289 - val_loss: 1.1215 - val_acc: 0.7240\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1220 - acc: 0.7279 - val_loss: 1.1221 - val_acc: 0.7230\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1196 - acc: 0.7299 - val_loss: 1.1177 - val_acc: 0.7250\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1176 - acc: 0.7300 - val_loss: 1.1167 - val_acc: 0.7260\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1152 - acc: 0.7308 - val_loss: 1.1157 - val_acc: 0.7300\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1133 - acc: 0.7295 - val_loss: 1.1136 - val_acc: 0.7260\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1110 - acc: 0.7289 - val_loss: 1.1084 - val_acc: 0.7240\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1086 - acc: 0.7309 - val_loss: 1.1088 - val_acc: 0.7200\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1069 - acc: 0.7328 - val_loss: 1.1091 - val_acc: 0.7240\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1046 - acc: 0.7312 - val_loss: 1.1076 - val_acc: 0.7250\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1026 - acc: 0.7319 - val_loss: 1.1021 - val_acc: 0.7240\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1006 - acc: 0.7333 - val_loss: 1.1008 - val_acc: 0.7250\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0986 - acc: 0.7316 - val_loss: 1.0968 - val_acc: 0.7270\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0968 - acc: 0.7328 - val_loss: 1.0967 - val_acc: 0.7250\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0951 - acc: 0.7316 - val_loss: 1.0923 - val_acc: 0.7270\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0933 - acc: 0.7327 - val_loss: 1.0917 - val_acc: 0.7290\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0912 - acc: 0.7329 - val_loss: 1.0925 - val_acc: 0.7290\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0893 - acc: 0.7321 - val_loss: 1.0893 - val_acc: 0.7270\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0876 - acc: 0.7341 - val_loss: 1.0866 - val_acc: 0.7250\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0858 - acc: 0.7325 - val_loss: 1.0884 - val_acc: 0.7270\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0841 - acc: 0.7345 - val_loss: 1.0833 - val_acc: 0.7290\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0827 - acc: 0.7345 - val_loss: 1.0886 - val_acc: 0.7320\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0806 - acc: 0.7351 - val_loss: 1.0830 - val_acc: 0.7290\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0794 - acc: 0.7339 - val_loss: 1.0807 - val_acc: 0.7320\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0774 - acc: 0.7351 - val_loss: 1.0862 - val_acc: 0.7270\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0760 - acc: 0.7360 - val_loss: 1.0879 - val_acc: 0.7250\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0749 - acc: 0.7357 - val_loss: 1.0749 - val_acc: 0.7280\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0730 - acc: 0.7365 - val_loss: 1.0741 - val_acc: 0.7290\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0716 - acc: 0.7357 - val_loss: 1.0730 - val_acc: 0.7300\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0693 - acc: 0.7371 - val_loss: 1.0753 - val_acc: 0.7280\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0681 - acc: 0.7385 - val_loss: 1.0727 - val_acc: 0.7320\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0667 - acc: 0.7373 - val_loss: 1.0703 - val_acc: 0.7340\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0652 - acc: 0.7376 - val_loss: 1.0709 - val_acc: 0.7290\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0640 - acc: 0.7381 - val_loss: 1.0679 - val_acc: 0.7320\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0627 - acc: 0.7376 - val_loss: 1.0672 - val_acc: 0.7330\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0617 - acc: 0.7395 - val_loss: 1.0634 - val_acc: 0.7300\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0596 - acc: 0.7379 - val_loss: 1.0650 - val_acc: 0.7320\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0580 - acc: 0.7396 - val_loss: 1.0622 - val_acc: 0.7300\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0562 - acc: 0.7391 - val_loss: 1.0619 - val_acc: 0.7290\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0554 - acc: 0.7399 - val_loss: 1.0663 - val_acc: 0.7350\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0545 - acc: 0.7392 - val_loss: 1.0592 - val_acc: 0.7330\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0530 - acc: 0.7404 - val_loss: 1.0578 - val_acc: 0.7310\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0517 - acc: 0.7412 - val_loss: 1.0557 - val_acc: 0.7320\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0501 - acc: 0.7417 - val_loss: 1.0560 - val_acc: 0.7350\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0495 - acc: 0.7403 - val_loss: 1.0558 - val_acc: 0.7300\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0471 - acc: 0.7407 - val_loss: 1.0583 - val_acc: 0.7280\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0465 - acc: 0.7404 - val_loss: 1.0509 - val_acc: 0.7360\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0449 - acc: 0.7408 - val_loss: 1.0510 - val_acc: 0.7340\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0433 - acc: 0.7420 - val_loss: 1.0536 - val_acc: 0.7310\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0427 - acc: 0.7417 - val_loss: 1.0478 - val_acc: 0.7280\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0409 - acc: 0.7417 - val_loss: 1.0461 - val_acc: 0.7320\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0394 - acc: 0.7432 - val_loss: 1.0463 - val_acc: 0.7270\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0393 - acc: 0.7419 - val_loss: 1.0464 - val_acc: 0.7350\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0371 - acc: 0.7428 - val_loss: 1.0471 - val_acc: 0.7340\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0360 - acc: 0.7420 - val_loss: 1.0429 - val_acc: 0.7320\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0341 - acc: 0.7416 - val_loss: 1.0416 - val_acc: 0.7330\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0331 - acc: 0.7428 - val_loss: 1.0445 - val_acc: 0.7340\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0322 - acc: 0.7440 - val_loss: 1.0409 - val_acc: 0.7350\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0311 - acc: 0.7448 - val_loss: 1.0405 - val_acc: 0.7360\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0295 - acc: 0.7433 - val_loss: 1.0380 - val_acc: 0.7360\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0280 - acc: 0.7452 - val_loss: 1.0375 - val_acc: 0.7290\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0274 - acc: 0.7441 - val_loss: 1.0346 - val_acc: 0.7340\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0255 - acc: 0.7452 - val_loss: 1.0346 - val_acc: 0.7340\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0239 - acc: 0.7445 - val_loss: 1.0323 - val_acc: 0.7320\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0233 - acc: 0.7449 - val_loss: 1.0389 - val_acc: 0.7320\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0221 - acc: 0.7448 - val_loss: 1.0314 - val_acc: 0.7260\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0206 - acc: 0.7455 - val_loss: 1.0342 - val_acc: 0.7290\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0191 - acc: 0.7449 - val_loss: 1.0297 - val_acc: 0.7340\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0187 - acc: 0.7457 - val_loss: 1.0254 - val_acc: 0.7310\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0178 - acc: 0.7451 - val_loss: 1.0321 - val_acc: 0.7260\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0167 - acc: 0.7467 - val_loss: 1.0279 - val_acc: 0.7390\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0154 - acc: 0.7468 - val_loss: 1.0237 - val_acc: 0.7320\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0133 - acc: 0.7476 - val_loss: 1.0264 - val_acc: 0.7310\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0129 - acc: 0.7467 - val_loss: 1.0263 - val_acc: 0.7300\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0117 - acc: 0.7473 - val_loss: 1.0280 - val_acc: 0.7330\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0112 - acc: 0.7480 - val_loss: 1.0319 - val_acc: 0.7290\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0103 - acc: 0.7464 - val_loss: 1.0196 - val_acc: 0.7390\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0083 - acc: 0.7475 - val_loss: 1.0197 - val_acc: 0.7320\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0078 - acc: 0.7491 - val_loss: 1.0365 - val_acc: 0.7280\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0068 - acc: 0.7479 - val_loss: 1.0191 - val_acc: 0.7300\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0055 - acc: 0.7489 - val_loss: 1.0205 - val_acc: 0.7330\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0044 - acc: 0.7483 - val_loss: 1.0199 - val_acc: 0.7360\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0042 - acc: 0.7477 - val_loss: 1.0232 - val_acc: 0.7350\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0032 - acc: 0.7492 - val_loss: 1.0132 - val_acc: 0.7310\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0016 - acc: 0.7468 - val_loss: 1.0147 - val_acc: 0.7320\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0004 - acc: 0.7492 - val_loss: 1.0151 - val_acc: 0.7320\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9991 - acc: 0.7497 - val_loss: 1.0119 - val_acc: 0.7290\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9981 - acc: 0.7493 - val_loss: 1.0114 - val_acc: 0.7350\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9985 - acc: 0.7501 - val_loss: 1.0105 - val_acc: 0.7310\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9965 - acc: 0.7505 - val_loss: 1.0086 - val_acc: 0.7360\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9963 - acc: 0.7480 - val_loss: 1.0096 - val_acc: 0.7310\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9946 - acc: 0.7497 - val_loss: 1.0135 - val_acc: 0.7360\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9950 - acc: 0.7505 - val_loss: 1.0191 - val_acc: 0.7370\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9942 - acc: 0.7503 - val_loss: 1.0052 - val_acc: 0.7340\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9928 - acc: 0.7504 - val_loss: 1.0165 - val_acc: 0.7330\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9913 - acc: 0.7513 - val_loss: 1.0014 - val_acc: 0.7350\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9901 - acc: 0.7489 - val_loss: 1.0026 - val_acc: 0.7350\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9894 - acc: 0.7497 - val_loss: 1.0030 - val_acc: 0.7410\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9885 - acc: 0.7495 - val_loss: 1.0041 - val_acc: 0.7340\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9885 - acc: 0.7509 - val_loss: 1.0013 - val_acc: 0.7340\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9866 - acc: 0.7512 - val_loss: 1.0009 - val_acc: 0.7370\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9854 - acc: 0.7523 - val_loss: 0.9986 - val_acc: 0.7300\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9850 - acc: 0.7497 - val_loss: 1.0016 - val_acc: 0.7390\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9838 - acc: 0.7523 - val_loss: 1.0013 - val_acc: 0.7320\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9826 - acc: 0.7516 - val_loss: 0.9984 - val_acc: 0.7430\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9820 - acc: 0.7509 - val_loss: 0.9973 - val_acc: 0.7370\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9812 - acc: 0.7536 - val_loss: 0.9990 - val_acc: 0.7370\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9805 - acc: 0.7516 - val_loss: 1.0021 - val_acc: 0.7400\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9804 - acc: 0.7511 - val_loss: 0.9935 - val_acc: 0.7330\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9793 - acc: 0.7508 - val_loss: 0.9957 - val_acc: 0.7390\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9778 - acc: 0.7519 - val_loss: 0.9934 - val_acc: 0.7370\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9769 - acc: 0.7525 - val_loss: 0.9946 - val_acc: 0.7380\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9766 - acc: 0.7521 - val_loss: 0.9974 - val_acc: 0.7430\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9755 - acc: 0.7531 - val_loss: 0.9904 - val_acc: 0.7360\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9748 - acc: 0.7531 - val_loss: 0.9909 - val_acc: 0.7370\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9743 - acc: 0.7523 - val_loss: 0.9932 - val_acc: 0.7380\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9736 - acc: 0.7556 - val_loss: 0.9958 - val_acc: 0.7380\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9721 - acc: 0.7544 - val_loss: 0.9999 - val_acc: 0.7410\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9716 - acc: 0.7541 - val_loss: 0.9939 - val_acc: 0.7370\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9712 - acc: 0.7528 - val_loss: 0.9891 - val_acc: 0.7410\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9698 - acc: 0.7540 - val_loss: 0.9869 - val_acc: 0.7380\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9700 - acc: 0.7551 - val_loss: 0.9915 - val_acc: 0.7380\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9683 - acc: 0.7575 - val_loss: 0.9886 - val_acc: 0.7330\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9675 - acc: 0.7545 - val_loss: 0.9852 - val_acc: 0.7430\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9672 - acc: 0.7555 - val_loss: 0.9882 - val_acc: 0.7360\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9669 - acc: 0.7543 - val_loss: 0.9883 - val_acc: 0.7430\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9653 - acc: 0.7541 - val_loss: 0.9882 - val_acc: 0.7430\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9651 - acc: 0.7535 - val_loss: 0.9851 - val_acc: 0.7390\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9637 - acc: 0.7569 - val_loss: 0.9842 - val_acc: 0.7400\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9636 - acc: 0.7555 - val_loss: 0.9845 - val_acc: 0.7400\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9629 - acc: 0.7549 - val_loss: 0.9853 - val_acc: 0.7440\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9622 - acc: 0.7557 - val_loss: 0.9849 - val_acc: 0.7410\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9620 - acc: 0.7553 - val_loss: 0.9866 - val_acc: 0.7400\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9608 - acc: 0.7579 - val_loss: 0.9841 - val_acc: 0.7410\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9597 - acc: 0.7576 - val_loss: 0.9848 - val_acc: 0.7410\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9602 - acc: 0.7583 - val_loss: 0.9780 - val_acc: 0.7400\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9591 - acc: 0.7540 - val_loss: 0.9797 - val_acc: 0.7410\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9574 - acc: 0.7559 - val_loss: 0.9794 - val_acc: 0.7440\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9576 - acc: 0.7557 - val_loss: 0.9761 - val_acc: 0.7350\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9580 - acc: 0.7569 - val_loss: 0.9752 - val_acc: 0.7400\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9548 - acc: 0.7575 - val_loss: 0.9753 - val_acc: 0.7400\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9543 - acc: 0.7585 - val_loss: 0.9754 - val_acc: 0.7380\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9530 - acc: 0.7561 - val_loss: 0.9760 - val_acc: 0.7450\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9526 - acc: 0.7580 - val_loss: 0.9816 - val_acc: 0.7440\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9525 - acc: 0.7584 - val_loss: 0.9751 - val_acc: 0.7420\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9521 - acc: 0.7581 - val_loss: 0.9733 - val_acc: 0.7470\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9514 - acc: 0.7601 - val_loss: 0.9761 - val_acc: 0.7410\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9508 - acc: 0.7588 - val_loss: 0.9729 - val_acc: 0.7400\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9502 - acc: 0.7567 - val_loss: 0.9772 - val_acc: 0.7390\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9484 - acc: 0.7595 - val_loss: 0.9813 - val_acc: 0.7390\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9484 - acc: 0.7588 - val_loss: 0.9723 - val_acc: 0.7450\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9473 - acc: 0.7600 - val_loss: 0.9753 - val_acc: 0.7420\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9475 - acc: 0.7587 - val_loss: 0.9704 - val_acc: 0.7400\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9464 - acc: 0.7592 - val_loss: 0.9691 - val_acc: 0.7420\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9458 - acc: 0.7591 - val_loss: 0.9721 - val_acc: 0.7430\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9452 - acc: 0.7613 - val_loss: 0.9709 - val_acc: 0.7380\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9439 - acc: 0.7611 - val_loss: 0.9684 - val_acc: 0.7440\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9439 - acc: 0.7589 - val_loss: 0.9652 - val_acc: 0.7430\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9428 - acc: 0.7603 - val_loss: 0.9755 - val_acc: 0.7380\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9422 - acc: 0.7613 - val_loss: 0.9655 - val_acc: 0.7380\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9415 - acc: 0.7589 - val_loss: 0.9755 - val_acc: 0.7460\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9420 - acc: 0.7608 - val_loss: 0.9658 - val_acc: 0.7360\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9408 - acc: 0.7601 - val_loss: 0.9707 - val_acc: 0.7420\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9398 - acc: 0.7613 - val_loss: 0.9684 - val_acc: 0.7460\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9398 - acc: 0.7600 - val_loss: 0.9641 - val_acc: 0.7430\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9386 - acc: 0.7615 - val_loss: 0.9672 - val_acc: 0.7380\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9379 - acc: 0.7620 - val_loss: 0.9620 - val_acc: 0.7400\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9371 - acc: 0.7627 - val_loss: 0.9641 - val_acc: 0.7440\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9368 - acc: 0.7627 - val_loss: 0.9662 - val_acc: 0.7490\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9354 - acc: 0.7611 - val_loss: 0.9641 - val_acc: 0.7430\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9352 - acc: 0.7613 - val_loss: 0.9602 - val_acc: 0.7450\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9347 - acc: 0.7612 - val_loss: 0.9645 - val_acc: 0.7390\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9352 - acc: 0.7623 - val_loss: 0.9646 - val_acc: 0.7450\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9325 - acc: 0.7648 - val_loss: 0.9609 - val_acc: 0.7400\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9323 - acc: 0.7637 - val_loss: 0.9739 - val_acc: 0.7420\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9321 - acc: 0.7631 - val_loss: 0.9624 - val_acc: 0.7490\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9311 - acc: 0.7631 - val_loss: 0.9590 - val_acc: 0.7450\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9320 - acc: 0.7632 - val_loss: 0.9626 - val_acc: 0.7410\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9308 - acc: 0.7629 - val_loss: 0.9557 - val_acc: 0.7410\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9289 - acc: 0.7628 - val_loss: 0.9580 - val_acc: 0.7450\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9297 - acc: 0.7671 - val_loss: 0.9614 - val_acc: 0.7410\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9287 - acc: 0.7647 - val_loss: 0.9553 - val_acc: 0.7410\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9275 - acc: 0.7673 - val_loss: 0.9584 - val_acc: 0.7450\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9277 - acc: 0.7653 - val_loss: 0.9678 - val_acc: 0.7510\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9269 - acc: 0.7652 - val_loss: 0.9683 - val_acc: 0.7430\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9269 - acc: 0.7655 - val_loss: 0.9623 - val_acc: 0.7410\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9253 - acc: 0.7672 - val_loss: 0.9678 - val_acc: 0.7430\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9252 - acc: 0.7655 - val_loss: 0.9588 - val_acc: 0.7440\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9249 - acc: 0.7647 - val_loss: 0.9603 - val_acc: 0.7370\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9254 - acc: 0.7672 - val_loss: 0.9528 - val_acc: 0.7440\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9237 - acc: 0.7675 - val_loss: 0.9565 - val_acc: 0.7480\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9232 - acc: 0.7667 - val_loss: 0.9531 - val_acc: 0.7450\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9223 - acc: 0.7672 - val_loss: 0.9557 - val_acc: 0.7410\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9219 - acc: 0.7673 - val_loss: 0.9645 - val_acc: 0.7430\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9224 - acc: 0.7661 - val_loss: 0.9533 - val_acc: 0.7440\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9197 - acc: 0.7668 - val_loss: 0.9505 - val_acc: 0.7390\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9200 - acc: 0.7667 - val_loss: 0.9510 - val_acc: 0.7430\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9192 - acc: 0.7677 - val_loss: 0.9665 - val_acc: 0.7360\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9199 - acc: 0.7659 - val_loss: 0.9560 - val_acc: 0.7450\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9188 - acc: 0.7687 - val_loss: 0.9523 - val_acc: 0.7460\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9179 - acc: 0.7680 - val_loss: 0.9598 - val_acc: 0.7460\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9172 - acc: 0.7673 - val_loss: 0.9572 - val_acc: 0.7500\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9162 - acc: 0.7689 - val_loss: 0.9546 - val_acc: 0.7470\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9182 - acc: 0.7669 - val_loss: 0.9544 - val_acc: 0.7480\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9156 - acc: 0.7693 - val_loss: 0.9519 - val_acc: 0.7470\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9148 - acc: 0.7684 - val_loss: 0.9500 - val_acc: 0.7450\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9150 - acc: 0.7679 - val_loss: 0.9541 - val_acc: 0.7420\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9146 - acc: 0.7675 - val_loss: 0.9515 - val_acc: 0.7470\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9128 - acc: 0.7692 - val_loss: 0.9528 - val_acc: 0.7410\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9135 - acc: 0.7685 - val_loss: 0.9485 - val_acc: 0.7480\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9108 - acc: 0.7700 - val_loss: 0.9465 - val_acc: 0.7480\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9114 - acc: 0.7695 - val_loss: 0.9624 - val_acc: 0.7400\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9122 - acc: 0.7689 - val_loss: 0.9746 - val_acc: 0.7340\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9122 - acc: 0.7704 - val_loss: 0.9497 - val_acc: 0.7460\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9098 - acc: 0.7687 - val_loss: 0.9501 - val_acc: 0.7470\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9101 - acc: 0.7700 - val_loss: 0.9555 - val_acc: 0.7420\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9097 - acc: 0.7695 - val_loss: 0.9472 - val_acc: 0.7440\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9078 - acc: 0.7701 - val_loss: 0.9539 - val_acc: 0.7450\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9084 - acc: 0.7715 - val_loss: 0.9437 - val_acc: 0.7470\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9081 - acc: 0.7685 - val_loss: 0.9510 - val_acc: 0.7450\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9075 - acc: 0.7719 - val_loss: 0.9463 - val_acc: 0.7420\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9062 - acc: 0.7692 - val_loss: 0.9500 - val_acc: 0.7400\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9048 - acc: 0.7708 - val_loss: 0.9568 - val_acc: 0.7430\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9069 - acc: 0.7704 - val_loss: 0.9455 - val_acc: 0.7440\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9044 - acc: 0.7720 - val_loss: 0.9505 - val_acc: 0.7440\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9042 - acc: 0.7696 - val_loss: 0.9432 - val_acc: 0.7370\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9030 - acc: 0.7713 - val_loss: 0.9438 - val_acc: 0.7500\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9025 - acc: 0.7720 - val_loss: 0.9472 - val_acc: 0.7460\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9029 - acc: 0.7728 - val_loss: 0.9426 - val_acc: 0.7460\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9021 - acc: 0.7701 - val_loss: 0.9548 - val_acc: 0.7410\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9023 - acc: 0.7712 - val_loss: 0.9415 - val_acc: 0.7460\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9020 - acc: 0.7713 - val_loss: 0.9493 - val_acc: 0.7470\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9010 - acc: 0.7719 - val_loss: 0.9411 - val_acc: 0.7480\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9007 - acc: 0.7700 - val_loss: 0.9385 - val_acc: 0.7490\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9008 - acc: 0.7720 - val_loss: 0.9362 - val_acc: 0.7470\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8985 - acc: 0.7704 - val_loss: 0.9375 - val_acc: 0.7480\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8980 - acc: 0.7740 - val_loss: 0.9366 - val_acc: 0.7490\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8977 - acc: 0.7751 - val_loss: 0.9360 - val_acc: 0.7450\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8982 - acc: 0.7733 - val_loss: 0.9507 - val_acc: 0.7480\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8982 - acc: 0.7713 - val_loss: 0.9383 - val_acc: 0.7470\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8963 - acc: 0.7712 - val_loss: 0.9529 - val_acc: 0.7500\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8962 - acc: 0.7740 - val_loss: 0.9548 - val_acc: 0.7490\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8960 - acc: 0.7741 - val_loss: 0.9380 - val_acc: 0.7430\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8952 - acc: 0.7729 - val_loss: 0.9352 - val_acc: 0.7460\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8945 - acc: 0.7741 - val_loss: 0.9368 - val_acc: 0.7480\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8947 - acc: 0.7744 - val_loss: 0.9393 - val_acc: 0.7480\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8940 - acc: 0.7747 - val_loss: 0.9354 - val_acc: 0.7450\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8927 - acc: 0.7753 - val_loss: 0.9537 - val_acc: 0.7410\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8933 - acc: 0.7731 - val_loss: 0.9435 - val_acc: 0.7500\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8929 - acc: 0.7732 - val_loss: 0.9353 - val_acc: 0.7530\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8935 - acc: 0.7740 - val_loss: 0.9351 - val_acc: 0.7470\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8912 - acc: 0.7765 - val_loss: 0.9418 - val_acc: 0.7470\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8914 - acc: 0.7747 - val_loss: 0.9436 - val_acc: 0.7490\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8900 - acc: 0.7733 - val_loss: 0.9334 - val_acc: 0.7490\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8900 - acc: 0.7771 - val_loss: 0.9387 - val_acc: 0.7470\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8895 - acc: 0.7748 - val_loss: 0.9341 - val_acc: 0.7470\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8912 - acc: 0.7732 - val_loss: 0.9402 - val_acc: 0.7470\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8892 - acc: 0.7741 - val_loss: 0.9418 - val_acc: 0.7470\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8892 - acc: 0.7769 - val_loss: 0.9438 - val_acc: 0.7400\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8899 - acc: 0.7716 - val_loss: 0.9487 - val_acc: 0.7440\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8861 - acc: 0.7761 - val_loss: 0.9386 - val_acc: 0.7420\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8866 - acc: 0.7767 - val_loss: 0.9432 - val_acc: 0.7450\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8872 - acc: 0.7760 - val_loss: 0.9291 - val_acc: 0.7540\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8848 - acc: 0.7769 - val_loss: 0.9416 - val_acc: 0.7490\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8871 - acc: 0.7745 - val_loss: 0.9314 - val_acc: 0.7480\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8844 - acc: 0.7753 - val_loss: 0.9425 - val_acc: 0.7500\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8841 - acc: 0.7757 - val_loss: 0.9329 - val_acc: 0.7500\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8846 - acc: 0.7748 - val_loss: 0.9428 - val_acc: 0.7470\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8843 - acc: 0.7780 - val_loss: 0.9546 - val_acc: 0.7440\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8838 - acc: 0.7779 - val_loss: 0.9493 - val_acc: 0.7460\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8824 - acc: 0.7785 - val_loss: 0.9386 - val_acc: 0.7500\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8817 - acc: 0.7769 - val_loss: 0.9284 - val_acc: 0.7500\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8823 - acc: 0.7747 - val_loss: 0.9447 - val_acc: 0.7460\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8828 - acc: 0.7743 - val_loss: 0.9286 - val_acc: 0.7470\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8809 - acc: 0.7777 - val_loss: 0.9375 - val_acc: 0.7520\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8795 - acc: 0.7788 - val_loss: 0.9405 - val_acc: 0.7500\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8816 - acc: 0.7788 - val_loss: 0.9274 - val_acc: 0.7460\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8789 - acc: 0.7771 - val_loss: 0.9283 - val_acc: 0.7520\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8792 - acc: 0.7768 - val_loss: 0.9343 - val_acc: 0.7560\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8799 - acc: 0.7764 - val_loss: 0.9334 - val_acc: 0.7430\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8786 - acc: 0.7793 - val_loss: 0.9257 - val_acc: 0.7490\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8773 - acc: 0.7773 - val_loss: 0.9276 - val_acc: 0.7470\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8785 - acc: 0.7776 - val_loss: 0.9311 - val_acc: 0.7500\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8762 - acc: 0.7769 - val_loss: 0.9324 - val_acc: 0.7520\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8763 - acc: 0.7804 - val_loss: 0.9266 - val_acc: 0.7500\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8762 - acc: 0.7796 - val_loss: 0.9320 - val_acc: 0.7450\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8758 - acc: 0.7805 - val_loss: 0.9412 - val_acc: 0.7430\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8761 - acc: 0.7777 - val_loss: 0.9301 - val_acc: 0.7430\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8760 - acc: 0.7785 - val_loss: 0.9245 - val_acc: 0.7540\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8753 - acc: 0.7793 - val_loss: 0.9273 - val_acc: 0.7460\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8748 - acc: 0.7773 - val_loss: 0.9258 - val_acc: 0.7470\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8737 - acc: 0.7788 - val_loss: 0.9237 - val_acc: 0.7500\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8737 - acc: 0.7785 - val_loss: 0.9291 - val_acc: 0.7520\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8724 - acc: 0.7819 - val_loss: 0.9337 - val_acc: 0.7510\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8729 - acc: 0.7821 - val_loss: 0.9240 - val_acc: 0.7500\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8721 - acc: 0.7811 - val_loss: 0.9224 - val_acc: 0.7540\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8706 - acc: 0.7803 - val_loss: 0.9249 - val_acc: 0.7500\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8730 - acc: 0.7815 - val_loss: 0.9266 - val_acc: 0.7470\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8716 - acc: 0.7801 - val_loss: 0.9251 - val_acc: 0.7510\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8717 - acc: 0.7791 - val_loss: 0.9213 - val_acc: 0.7510\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8708 - acc: 0.7784 - val_loss: 0.9267 - val_acc: 0.7490\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8704 - acc: 0.7803 - val_loss: 0.9225 - val_acc: 0.7500\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8686 - acc: 0.7803 - val_loss: 0.9206 - val_acc: 0.7470\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8684 - acc: 0.7821 - val_loss: 0.9616 - val_acc: 0.7380\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8695 - acc: 0.7803 - val_loss: 0.9269 - val_acc: 0.7500\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8682 - acc: 0.7821 - val_loss: 0.9440 - val_acc: 0.7460\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8706 - acc: 0.7823 - val_loss: 0.9232 - val_acc: 0.7490\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8669 - acc: 0.7816 - val_loss: 0.9309 - val_acc: 0.7480\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8671 - acc: 0.7805 - val_loss: 0.9657 - val_acc: 0.7360\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8694 - acc: 0.7811 - val_loss: 0.9218 - val_acc: 0.7530\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8682 - acc: 0.7827 - val_loss: 0.9225 - val_acc: 0.7490\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8675 - acc: 0.7809 - val_loss: 0.9300 - val_acc: 0.7450\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8658 - acc: 0.7816 - val_loss: 0.9219 - val_acc: 0.7490\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8663 - acc: 0.7807 - val_loss: 0.9297 - val_acc: 0.7520\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8659 - acc: 0.7807 - val_loss: 0.9338 - val_acc: 0.7480\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8670 - acc: 0.7808 - val_loss: 0.9298 - val_acc: 0.7500\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8656 - acc: 0.7816 - val_loss: 0.9242 - val_acc: 0.7510\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8633 - acc: 0.7824 - val_loss: 0.9200 - val_acc: 0.7510\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8636 - acc: 0.7813 - val_loss: 0.9334 - val_acc: 0.7470\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8663 - acc: 0.7820 - val_loss: 0.9207 - val_acc: 0.7500\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8631 - acc: 0.7831 - val_loss: 0.9234 - val_acc: 0.7510\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8642 - acc: 0.7783 - val_loss: 0.9250 - val_acc: 0.7480\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8638 - acc: 0.7813 - val_loss: 0.9340 - val_acc: 0.7460\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8633 - acc: 0.7837 - val_loss: 0.9453 - val_acc: 0.7470\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8629 - acc: 0.7824 - val_loss: 0.9673 - val_acc: 0.7350\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8630 - acc: 0.7807 - val_loss: 0.9219 - val_acc: 0.7540\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8622 - acc: 0.7825 - val_loss: 0.9233 - val_acc: 0.7460\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8597 - acc: 0.7816 - val_loss: 0.9209 - val_acc: 0.7510\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8593 - acc: 0.7845 - val_loss: 0.9248 - val_acc: 0.7480\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8599 - acc: 0.7835 - val_loss: 0.9397 - val_acc: 0.7440\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8595 - acc: 0.7841 - val_loss: 0.9215 - val_acc: 0.7520\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8597 - acc: 0.7851 - val_loss: 0.9332 - val_acc: 0.7530\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8597 - acc: 0.7828 - val_loss: 0.9245 - val_acc: 0.7460\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8582 - acc: 0.7821 - val_loss: 0.9193 - val_acc: 0.7520\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8596 - acc: 0.7825 - val_loss: 0.9580 - val_acc: 0.7420\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8604 - acc: 0.7827 - val_loss: 0.9212 - val_acc: 0.7500\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8597 - acc: 0.7816 - val_loss: 0.9289 - val_acc: 0.7420\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8575 - acc: 0.7865 - val_loss: 0.9185 - val_acc: 0.7520\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8570 - acc: 0.7820 - val_loss: 0.9209 - val_acc: 0.7450\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8561 - acc: 0.7863 - val_loss: 0.9736 - val_acc: 0.7350\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8607 - acc: 0.7825 - val_loss: 0.9300 - val_acc: 0.7460\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8568 - acc: 0.7868 - val_loss: 0.9175 - val_acc: 0.7500\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8564 - acc: 0.7851 - val_loss: 0.9220 - val_acc: 0.7500\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8558 - acc: 0.7845 - val_loss: 0.9213 - val_acc: 0.7480\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8539 - acc: 0.7849 - val_loss: 0.9561 - val_acc: 0.7390\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8565 - acc: 0.7803 - val_loss: 0.9188 - val_acc: 0.7510\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8546 - acc: 0.7855 - val_loss: 0.9275 - val_acc: 0.7480\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8530 - acc: 0.7859 - val_loss: 0.9179 - val_acc: 0.7490\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8547 - acc: 0.7848 - val_loss: 0.9162 - val_acc: 0.7460\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8537 - acc: 0.7869 - val_loss: 0.9284 - val_acc: 0.7490\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8531 - acc: 0.7873 - val_loss: 0.9179 - val_acc: 0.7490\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8533 - acc: 0.7857 - val_loss: 0.9175 - val_acc: 0.7460\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8523 - acc: 0.7871 - val_loss: 0.9517 - val_acc: 0.7400\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8560 - acc: 0.7873 - val_loss: 0.9295 - val_acc: 0.7500\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8510 - acc: 0.7881 - val_loss: 0.9209 - val_acc: 0.7470\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8532 - acc: 0.7844 - val_loss: 0.9486 - val_acc: 0.7450\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8527 - acc: 0.7864 - val_loss: 0.9529 - val_acc: 0.7430\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8554 - acc: 0.7857 - val_loss: 0.9170 - val_acc: 0.7500\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8506 - acc: 0.7848 - val_loss: 0.9206 - val_acc: 0.7540\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8512 - acc: 0.7876 - val_loss: 0.9226 - val_acc: 0.7510\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8528 - acc: 0.7867 - val_loss: 0.9260 - val_acc: 0.7540\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8491 - acc: 0.7892 - val_loss: 0.9178 - val_acc: 0.7490\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8489 - acc: 0.7899 - val_loss: 0.9142 - val_acc: 0.7480\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8490 - acc: 0.7891 - val_loss: 0.9139 - val_acc: 0.7490\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8483 - acc: 0.7901 - val_loss: 0.9283 - val_acc: 0.7450\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8486 - acc: 0.7913 - val_loss: 0.9256 - val_acc: 0.7550\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8479 - acc: 0.7900 - val_loss: 0.9142 - val_acc: 0.7510\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8514 - acc: 0.7848 - val_loss: 0.9424 - val_acc: 0.7410\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8484 - acc: 0.7892 - val_loss: 0.9397 - val_acc: 0.7450\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8497 - acc: 0.7876 - val_loss: 0.9154 - val_acc: 0.7450\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8454 - acc: 0.7892 - val_loss: 0.9406 - val_acc: 0.7400\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8467 - acc: 0.7885 - val_loss: 0.9179 - val_acc: 0.7550\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8453 - acc: 0.7885 - val_loss: 0.9183 - val_acc: 0.7560\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8458 - acc: 0.7893 - val_loss: 0.9153 - val_acc: 0.7540\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8454 - acc: 0.7887 - val_loss: 0.9311 - val_acc: 0.7510\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8452 - acc: 0.7887 - val_loss: 0.9140 - val_acc: 0.7520\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8465 - acc: 0.7893 - val_loss: 0.9136 - val_acc: 0.7460\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8443 - acc: 0.7892 - val_loss: 0.9274 - val_acc: 0.7440\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8480 - acc: 0.7895 - val_loss: 0.9227 - val_acc: 0.7500\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8453 - acc: 0.7883 - val_loss: 0.9113 - val_acc: 0.7490\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8429 - acc: 0.7935 - val_loss: 0.9146 - val_acc: 0.7490\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8432 - acc: 0.7940 - val_loss: 0.9180 - val_acc: 0.7530\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8444 - acc: 0.7920 - val_loss: 0.9158 - val_acc: 0.7500\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8441 - acc: 0.7911 - val_loss: 0.9108 - val_acc: 0.7540\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8421 - acc: 0.7913 - val_loss: 0.9143 - val_acc: 0.7510\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8424 - acc: 0.7931 - val_loss: 0.9151 - val_acc: 0.7480\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8440 - acc: 0.7875 - val_loss: 0.9103 - val_acc: 0.7480\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8425 - acc: 0.7897 - val_loss: 0.9153 - val_acc: 0.7500\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8424 - acc: 0.7935 - val_loss: 0.9092 - val_acc: 0.7500\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8428 - acc: 0.7896 - val_loss: 0.9171 - val_acc: 0.7490\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8392 - acc: 0.7923 - val_loss: 0.9311 - val_acc: 0.7470\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8399 - acc: 0.7945 - val_loss: 0.9184 - val_acc: 0.7520\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8391 - acc: 0.7919 - val_loss: 0.9092 - val_acc: 0.7520\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8418 - acc: 0.7912 - val_loss: 0.9233 - val_acc: 0.7440\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8398 - acc: 0.7941 - val_loss: 0.9160 - val_acc: 0.7490\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8421 - acc: 0.7933 - val_loss: 0.9108 - val_acc: 0.7470\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8387 - acc: 0.7925 - val_loss: 0.9318 - val_acc: 0.7440\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8378 - acc: 0.7940 - val_loss: 0.9135 - val_acc: 0.7470\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8363 - acc: 0.7955 - val_loss: 0.9081 - val_acc: 0.7510\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8405 - acc: 0.7911 - val_loss: 0.9137 - val_acc: 0.7510\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8398 - acc: 0.7929 - val_loss: 0.9107 - val_acc: 0.7520\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8370 - acc: 0.7923 - val_loss: 0.9348 - val_acc: 0.7440\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8388 - acc: 0.7919 - val_loss: 0.9074 - val_acc: 0.7510\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8380 - acc: 0.7919 - val_loss: 0.9130 - val_acc: 0.7490\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8382 - acc: 0.7939 - val_loss: 0.9214 - val_acc: 0.7500\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8387 - acc: 0.7940 - val_loss: 0.9210 - val_acc: 0.7550\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8375 - acc: 0.7953 - val_loss: 0.9140 - val_acc: 0.7520\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8378 - acc: 0.7925 - val_loss: 0.9101 - val_acc: 0.7540\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8364 - acc: 0.7948 - val_loss: 0.9282 - val_acc: 0.7530\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8365 - acc: 0.7952 - val_loss: 0.9105 - val_acc: 0.7510\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8371 - acc: 0.7955 - val_loss: 0.9197 - val_acc: 0.7400\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8372 - acc: 0.7949 - val_loss: 0.9095 - val_acc: 0.7520\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8358 - acc: 0.7924 - val_loss: 0.9141 - val_acc: 0.7540\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8341 - acc: 0.7976 - val_loss: 0.9097 - val_acc: 0.7490\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8347 - acc: 0.7948 - val_loss: 0.9223 - val_acc: 0.7500\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8338 - acc: 0.7933 - val_loss: 0.9398 - val_acc: 0.7430\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8357 - acc: 0.7940 - val_loss: 0.9244 - val_acc: 0.7470\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8339 - acc: 0.7977 - val_loss: 0.9449 - val_acc: 0.7470\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8379 - acc: 0.7944 - val_loss: 0.9314 - val_acc: 0.7520\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8331 - acc: 0.7973 - val_loss: 0.9105 - val_acc: 0.7530\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8387 - acc: 0.7920 - val_loss: 0.9158 - val_acc: 0.7490\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8315 - acc: 0.7960 - val_loss: 0.9210 - val_acc: 0.7420\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8336 - acc: 0.7957 - val_loss: 0.9367 - val_acc: 0.7490\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8331 - acc: 0.7960 - val_loss: 0.9189 - val_acc: 0.7510\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8362 - acc: 0.7947 - val_loss: 0.9190 - val_acc: 0.7500\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8343 - acc: 0.7944 - val_loss: 0.9141 - val_acc: 0.7530\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8311 - acc: 0.7976 - val_loss: 0.9178 - val_acc: 0.7510\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8330 - acc: 0.7973 - val_loss: 0.9235 - val_acc: 0.7440\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8344 - acc: 0.7961 - val_loss: 0.9202 - val_acc: 0.7540\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8294 - acc: 0.7972 - val_loss: 0.9076 - val_acc: 0.7530\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8322 - acc: 0.7979 - val_loss: 0.9252 - val_acc: 0.7550\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8335 - acc: 0.7948 - val_loss: 0.9237 - val_acc: 0.7500\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8309 - acc: 0.7972 - val_loss: 0.9176 - val_acc: 0.7560\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8319 - acc: 0.7980 - val_loss: 0.9104 - val_acc: 0.7510\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8293 - acc: 0.8009 - val_loss: 0.9181 - val_acc: 0.7460\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8291 - acc: 0.7991 - val_loss: 0.9258 - val_acc: 0.7580\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8321 - acc: 0.8011 - val_loss: 0.9188 - val_acc: 0.7480\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8298 - acc: 0.7987 - val_loss: 0.9096 - val_acc: 0.7540\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8282 - acc: 0.8012 - val_loss: 0.9121 - val_acc: 0.7580\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8303 - acc: 0.7992 - val_loss: 0.9307 - val_acc: 0.7460\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8298 - acc: 0.7980 - val_loss: 0.9064 - val_acc: 0.7520\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8264 - acc: 0.8011 - val_loss: 0.9094 - val_acc: 0.7530\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8285 - acc: 0.7984 - val_loss: 0.9104 - val_acc: 0.7480\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8277 - acc: 0.8007 - val_loss: 0.9065 - val_acc: 0.7560\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8321 - acc: 0.7976 - val_loss: 0.9197 - val_acc: 0.7430\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8292 - acc: 0.7992 - val_loss: 0.9146 - val_acc: 0.7530\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8286 - acc: 0.7976 - val_loss: 0.9261 - val_acc: 0.7400\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8306 - acc: 0.7984 - val_loss: 0.9034 - val_acc: 0.7520\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8247 - acc: 0.8017 - val_loss: 0.9097 - val_acc: 0.7550\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8288 - acc: 0.8013 - val_loss: 0.9150 - val_acc: 0.7510\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8255 - acc: 0.8011 - val_loss: 0.9171 - val_acc: 0.7510\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8288 - acc: 0.7980 - val_loss: 0.9130 - val_acc: 0.7540\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8275 - acc: 0.7969 - val_loss: 0.9589 - val_acc: 0.7380\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8299 - acc: 0.7955 - val_loss: 0.9811 - val_acc: 0.7390\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8278 - acc: 0.7987 - val_loss: 1.0120 - val_acc: 0.7180\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8401 - acc: 0.7917 - val_loss: 0.9058 - val_acc: 0.7540\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8272 - acc: 0.7975 - val_loss: 0.9816 - val_acc: 0.7370\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8308 - acc: 0.7992 - val_loss: 0.9169 - val_acc: 0.7560\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8243 - acc: 0.8036 - val_loss: 0.9152 - val_acc: 0.7550\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8291 - acc: 0.7991 - val_loss: 0.9177 - val_acc: 0.7520\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8234 - acc: 0.8017 - val_loss: 0.9213 - val_acc: 0.7530\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8250 - acc: 0.8004 - val_loss: 0.9175 - val_acc: 0.7510\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8235 - acc: 0.8016 - val_loss: 0.9181 - val_acc: 0.7500\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8256 - acc: 0.8015 - val_loss: 0.9539 - val_acc: 0.7480\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8239 - acc: 0.8023 - val_loss: 0.9468 - val_acc: 0.7440\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8258 - acc: 0.8028 - val_loss: 0.9590 - val_acc: 0.7460\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8280 - acc: 0.8007 - val_loss: 0.9165 - val_acc: 0.7510\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8227 - acc: 0.8016 - val_loss: 0.9179 - val_acc: 0.7540\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8255 - acc: 0.8020 - val_loss: 0.9078 - val_acc: 0.7540\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8247 - acc: 0.7980 - val_loss: 0.9174 - val_acc: 0.7460\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8258 - acc: 0.7993 - val_loss: 0.9129 - val_acc: 0.7580\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8234 - acc: 0.8025 - val_loss: 0.9123 - val_acc: 0.7500\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8225 - acc: 0.8011 - val_loss: 0.9088 - val_acc: 0.7530\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8200 - acc: 0.8028 - val_loss: 0.9087 - val_acc: 0.7530\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8206 - acc: 0.7995 - val_loss: 0.9595 - val_acc: 0.7410\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8216 - acc: 0.8051 - val_loss: 0.9172 - val_acc: 0.7530\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8228 - acc: 0.8033 - val_loss: 0.9113 - val_acc: 0.7470\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8213 - acc: 0.8048 - val_loss: 0.9166 - val_acc: 0.7540\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8198 - acc: 0.8033 - val_loss: 0.9241 - val_acc: 0.7540\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8201 - acc: 0.8040 - val_loss: 0.9194 - val_acc: 0.7450\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8199 - acc: 0.8025 - val_loss: 0.9267 - val_acc: 0.7490\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8242 - acc: 0.8017 - val_loss: 0.9394 - val_acc: 0.7560\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8238 - acc: 0.8049 - val_loss: 0.9251 - val_acc: 0.7500\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8201 - acc: 0.8009 - val_loss: 0.9200 - val_acc: 0.7570\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8211 - acc: 0.8056 - val_loss: 0.9086 - val_acc: 0.7550\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8182 - acc: 0.8068 - val_loss: 0.9330 - val_acc: 0.7500\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8265 - acc: 0.8000 - val_loss: 0.9179 - val_acc: 0.7560\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8209 - acc: 0.8045 - val_loss: 0.9067 - val_acc: 0.7490\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8203 - acc: 0.8061 - val_loss: 0.9120 - val_acc: 0.7510\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8217 - acc: 0.8008 - val_loss: 0.9153 - val_acc: 0.7570\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8186 - acc: 0.8060 - val_loss: 0.9210 - val_acc: 0.7540\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8259 - acc: 0.8013 - val_loss: 0.9041 - val_acc: 0.7530\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8287 - acc: 0.8016 - val_loss: 0.9214 - val_acc: 0.7530\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8191 - acc: 0.8055 - val_loss: 0.9134 - val_acc: 0.7510\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8207 - acc: 0.8045 - val_loss: 0.9336 - val_acc: 0.7500\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8217 - acc: 0.8021 - val_loss: 0.9203 - val_acc: 0.7500\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8207 - acc: 0.8060 - val_loss: 0.9103 - val_acc: 0.7550\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8208 - acc: 0.8055 - val_loss: 0.9031 - val_acc: 0.7500\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8149 - acc: 0.8053 - val_loss: 0.9053 - val_acc: 0.7560\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8170 - acc: 0.8073 - val_loss: 0.9084 - val_acc: 0.7480\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8197 - acc: 0.8011 - val_loss: 0.9144 - val_acc: 0.7510\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8132 - acc: 0.8049 - val_loss: 0.9384 - val_acc: 0.7460\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8207 - acc: 0.8019 - val_loss: 0.9064 - val_acc: 0.7520\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8176 - acc: 0.8047 - val_loss: 0.9144 - val_acc: 0.7530\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8131 - acc: 0.8039 - val_loss: 0.9546 - val_acc: 0.7450\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8193 - acc: 0.8039 - val_loss: 0.9196 - val_acc: 0.7560\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8137 - acc: 0.8087 - val_loss: 0.9094 - val_acc: 0.7500\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8176 - acc: 0.8073 - val_loss: 0.9103 - val_acc: 0.7600\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8137 - acc: 0.8053 - val_loss: 0.9092 - val_acc: 0.7510\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8134 - acc: 0.8043 - val_loss: 0.9138 - val_acc: 0.7570\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8139 - acc: 0.8076 - val_loss: 0.9254 - val_acc: 0.7540\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8210 - acc: 0.8044 - val_loss: 0.9117 - val_acc: 0.7560\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8170 - acc: 0.8080 - val_loss: 0.9238 - val_acc: 0.7480\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8163 - acc: 0.8049 - val_loss: 0.9241 - val_acc: 0.7510\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8118 - acc: 0.8092 - val_loss: 0.9160 - val_acc: 0.7580\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8166 - acc: 0.8060 - val_loss: 0.9126 - val_acc: 0.7530\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8197 - acc: 0.8040 - val_loss: 0.9123 - val_acc: 0.7530\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8180 - acc: 0.8068 - val_loss: 0.9113 - val_acc: 0.7580\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8145 - acc: 0.8063 - val_loss: 0.9130 - val_acc: 0.7480\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8246 - acc: 0.8045 - val_loss: 0.9348 - val_acc: 0.7540\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8240 - acc: 0.8043 - val_loss: 0.9172 - val_acc: 0.7520\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8164 - acc: 0.8059 - val_loss: 0.9234 - val_acc: 0.7520\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8145 - acc: 0.8047 - val_loss: 0.9093 - val_acc: 0.7630\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8100 - acc: 0.8064 - val_loss: 0.9063 - val_acc: 0.7600\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8111 - acc: 0.8087 - val_loss: 0.9142 - val_acc: 0.7490\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8142 - acc: 0.8077 - val_loss: 0.9014 - val_acc: 0.7540\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8191 - acc: 0.8051 - val_loss: 0.9198 - val_acc: 0.7580\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8226 - acc: 0.8037 - val_loss: 0.9077 - val_acc: 0.7560\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8107 - acc: 0.8071 - val_loss: 0.9323 - val_acc: 0.7580\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8189 - acc: 0.8063 - val_loss: 0.9182 - val_acc: 0.7620\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8156 - acc: 0.8061 - val_loss: 0.9068 - val_acc: 0.7520\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8098 - acc: 0.8068 - val_loss: 0.9074 - val_acc: 0.7600\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8071 - acc: 0.8125 - val_loss: 0.9078 - val_acc: 0.7560\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8262 - acc: 0.7992 - val_loss: 0.9093 - val_acc: 0.7550\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8100 - acc: 0.8112 - val_loss: 0.9072 - val_acc: 0.7530\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8130 - acc: 0.8068 - val_loss: 0.9380 - val_acc: 0.7520\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8089 - acc: 0.8108 - val_loss: 0.9046 - val_acc: 0.7560\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8065 - acc: 0.8107 - val_loss: 0.9455 - val_acc: 0.7490\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8311 - acc: 0.8015 - val_loss: 0.9073 - val_acc: 0.7560\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8137 - acc: 0.8071 - val_loss: 0.9606 - val_acc: 0.7420\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8168 - acc: 0.8057 - val_loss: 0.9047 - val_acc: 0.7530\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8100 - acc: 0.8119 - val_loss: 0.9459 - val_acc: 0.7420\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8259 - acc: 0.8003 - val_loss: 0.9650 - val_acc: 0.7410\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8136 - acc: 0.8092 - val_loss: 0.9200 - val_acc: 0.7510\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8074 - acc: 0.8112 - val_loss: 0.9219 - val_acc: 0.7670\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8140 - acc: 0.8093 - val_loss: 0.9055 - val_acc: 0.7530\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8158 - acc: 0.8051 - val_loss: 0.9112 - val_acc: 0.7530\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8079 - acc: 0.8108 - val_loss: 0.9053 - val_acc: 0.7570\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8117 - acc: 0.8069 - val_loss: 0.9090 - val_acc: 0.7510\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8059 - acc: 0.8112 - val_loss: 0.9113 - val_acc: 0.7560\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8072 - acc: 0.8124 - val_loss: 0.9513 - val_acc: 0.7400\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8121 - acc: 0.8080 - val_loss: 0.9415 - val_acc: 0.7430\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8100 - acc: 0.8065 - val_loss: 0.9050 - val_acc: 0.7520\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8117 - acc: 0.8080 - val_loss: 0.9337 - val_acc: 0.7560\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8128 - acc: 0.8099 - val_loss: 0.8997 - val_acc: 0.7570\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8073 - acc: 0.8111 - val_loss: 0.9207 - val_acc: 0.7570\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8168 - acc: 0.8073 - val_loss: 1.0376 - val_acc: 0.7370\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8295 - acc: 0.8040 - val_loss: 0.9153 - val_acc: 0.7560\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8060 - acc: 0.8129 - val_loss: 0.9183 - val_acc: 0.7580\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8095 - acc: 0.8083 - val_loss: 0.9581 - val_acc: 0.7520\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8199 - acc: 0.8053 - val_loss: 0.9227 - val_acc: 0.7540\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8031 - acc: 0.8143 - val_loss: 0.9325 - val_acc: 0.7550\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8069 - acc: 0.8087 - val_loss: 1.0386 - val_acc: 0.7340\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8202 - acc: 0.8060 - val_loss: 0.9112 - val_acc: 0.7650\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8143 - acc: 0.8071 - val_loss: 0.9043 - val_acc: 0.7570\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8062 - acc: 0.8123 - val_loss: 0.9768 - val_acc: 0.7410\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8087 - acc: 0.8087 - val_loss: 0.9123 - val_acc: 0.7620\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8153 - acc: 0.8057 - val_loss: 0.9137 - val_acc: 0.7550\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8046 - acc: 0.8093 - val_loss: 0.9105 - val_acc: 0.7540\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8076 - acc: 0.8109 - val_loss: 0.9098 - val_acc: 0.7650\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8050 - acc: 0.8107 - val_loss: 0.9280 - val_acc: 0.7480\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8026 - acc: 0.8116 - val_loss: 0.9076 - val_acc: 0.7630\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8021 - acc: 0.8135 - val_loss: 0.9574 - val_acc: 0.7600\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8047 - acc: 0.8119 - val_loss: 0.9248 - val_acc: 0.7540\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8093 - acc: 0.8103 - val_loss: 0.9453 - val_acc: 0.7500\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8112 - acc: 0.8079 - val_loss: 0.9231 - val_acc: 0.7520\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8067 - acc: 0.8103 - val_loss: 0.9357 - val_acc: 0.7540\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8112 - acc: 0.8093 - val_loss: 0.9115 - val_acc: 0.7640\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8022 - acc: 0.8127 - val_loss: 0.9482 - val_acc: 0.7490\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8042 - acc: 0.8109 - val_loss: 0.9003 - val_acc: 0.7590\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8036 - acc: 0.8125 - val_loss: 0.9211 - val_acc: 0.7500\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8092 - acc: 0.8077 - val_loss: 0.9364 - val_acc: 0.7440\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8084 - acc: 0.8127 - val_loss: 0.9812 - val_acc: 0.7430\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8000 - acc: 0.8131 - val_loss: 0.9080 - val_acc: 0.7610\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8003 - acc: 0.8159 - val_loss: 0.9976 - val_acc: 0.7330\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8167 - acc: 0.8124 - val_loss: 0.9207 - val_acc: 0.7670\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8025 - acc: 0.8129 - val_loss: 0.9358 - val_acc: 0.7480\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8088 - acc: 0.8088 - val_loss: 0.9179 - val_acc: 0.7640\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8115 - acc: 0.8095 - val_loss: 0.9023 - val_acc: 0.7600\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8076 - acc: 0.8141 - val_loss: 0.9687 - val_acc: 0.7420\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8161 - acc: 0.8089 - val_loss: 0.9188 - val_acc: 0.7570\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7986 - acc: 0.8147 - val_loss: 0.9444 - val_acc: 0.7580\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8075 - acc: 0.8151 - val_loss: 1.1374 - val_acc: 0.6710\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8647 - acc: 0.7928 - val_loss: 0.9081 - val_acc: 0.7580\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8066 - acc: 0.8161 - val_loss: 0.9421 - val_acc: 0.7610\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8032 - acc: 0.8159 - val_loss: 0.9961 - val_acc: 0.7430\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8078 - acc: 0.8112 - val_loss: 0.9259 - val_acc: 0.7600\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8067 - acc: 0.8109 - val_loss: 0.9841 - val_acc: 0.7360\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8029 - acc: 0.8151 - val_loss: 0.9179 - val_acc: 0.7500\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8086 - acc: 0.8107 - val_loss: 0.9457 - val_acc: 0.7570\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8070 - acc: 0.8143 - val_loss: 0.9009 - val_acc: 0.7560\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8028 - acc: 0.8143 - val_loss: 0.9149 - val_acc: 0.7580\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8110 - acc: 0.8132 - val_loss: 0.9077 - val_acc: 0.7700\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7966 - acc: 0.8184 - val_loss: 0.9171 - val_acc: 0.7540\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7988 - acc: 0.8181 - val_loss: 0.9157 - val_acc: 0.7540\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8118 - acc: 0.8100 - val_loss: 0.9240 - val_acc: 0.7550\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8020 - acc: 0.8104 - val_loss: 0.9436 - val_acc: 0.7560\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8347 - acc: 0.7984 - val_loss: 0.9434 - val_acc: 0.7530\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8316 - acc: 0.8007 - val_loss: 0.9108 - val_acc: 0.7570\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8155 - acc: 0.8079 - val_loss: 0.9156 - val_acc: 0.7600\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8007 - acc: 0.8147 - val_loss: 0.9086 - val_acc: 0.7520\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7943 - acc: 0.8185 - val_loss: 0.9325 - val_acc: 0.7410\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8008 - acc: 0.8125 - val_loss: 0.9352 - val_acc: 0.7560\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8102 - acc: 0.8100 - val_loss: 0.9258 - val_acc: 0.7560\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7944 - acc: 0.8185 - val_loss: 0.9030 - val_acc: 0.7610\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8082 - acc: 0.8135 - val_loss: 0.9131 - val_acc: 0.7630\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8016 - acc: 0.8141 - val_loss: 1.0959 - val_acc: 0.7040\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8089 - acc: 0.8113 - val_loss: 0.9220 - val_acc: 0.7570\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8032 - acc: 0.8116 - val_loss: 0.9092 - val_acc: 0.7540\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8112 - acc: 0.8075 - val_loss: 0.9035 - val_acc: 0.7590\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7953 - acc: 0.8181 - val_loss: 0.9052 - val_acc: 0.7530\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8044 - acc: 0.8131 - val_loss: 0.9715 - val_acc: 0.7490\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8175 - acc: 0.8080 - val_loss: 0.9237 - val_acc: 0.7600\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7935 - acc: 0.8203 - val_loss: 0.9155 - val_acc: 0.7590\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8011 - acc: 0.8127 - val_loss: 0.9160 - val_acc: 0.7500\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7956 - acc: 0.8177 - val_loss: 0.9526 - val_acc: 0.7610\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8035 - acc: 0.8149 - val_loss: 0.9030 - val_acc: 0.7670\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7933 - acc: 0.8167 - val_loss: 0.9048 - val_acc: 0.7620\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7989 - acc: 0.8171 - val_loss: 0.9326 - val_acc: 0.7640\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8075 - acc: 0.8109 - val_loss: 0.9509 - val_acc: 0.7560\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7942 - acc: 0.8193 - val_loss: 0.9043 - val_acc: 0.7670\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7997 - acc: 0.8167 - val_loss: 0.8991 - val_acc: 0.7620\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7964 - acc: 0.8180 - val_loss: 0.9167 - val_acc: 0.7670\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7944 - acc: 0.8176 - val_loss: 0.9267 - val_acc: 0.7530\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7911 - acc: 0.8203 - val_loss: 0.9225 - val_acc: 0.7630\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8081 - acc: 0.8089 - val_loss: 0.9130 - val_acc: 0.7580\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8020 - acc: 0.8135 - val_loss: 0.9034 - val_acc: 0.7580\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7941 - acc: 0.8181 - val_loss: 0.9303 - val_acc: 0.7610\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7961 - acc: 0.8152 - val_loss: 0.9118 - val_acc: 0.7620\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7920 - acc: 0.8219 - val_loss: 1.0008 - val_acc: 0.7380\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7948 - acc: 0.8169 - val_loss: 0.9212 - val_acc: 0.7560\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8002 - acc: 0.8163 - val_loss: 0.9280 - val_acc: 0.7660\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8168 - acc: 0.8087 - val_loss: 0.8971 - val_acc: 0.7610\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7892 - acc: 0.8217 - val_loss: 0.9130 - val_acc: 0.7620\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7950 - acc: 0.8179 - val_loss: 0.9024 - val_acc: 0.7570\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7860 - acc: 0.8233 - val_loss: 0.9055 - val_acc: 0.7570\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7980 - acc: 0.8148 - val_loss: 0.8983 - val_acc: 0.7630\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7975 - acc: 0.8159 - val_loss: 0.9128 - val_acc: 0.7530\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7873 - acc: 0.8252 - val_loss: 0.9128 - val_acc: 0.7590\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8044 - acc: 0.8144 - val_loss: 0.9375 - val_acc: 0.7580\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7956 - acc: 0.8205 - val_loss: 0.9018 - val_acc: 0.7690\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8069 - acc: 0.8149 - val_loss: 0.9032 - val_acc: 0.7670\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7952 - acc: 0.8171 - val_loss: 0.9166 - val_acc: 0.7590\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8045 - acc: 0.8145 - val_loss: 0.9194 - val_acc: 0.7600\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8052 - acc: 0.8121 - val_loss: 0.9260 - val_acc: 0.7580\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7932 - acc: 0.8227 - val_loss: 0.8986 - val_acc: 0.7610\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7978 - acc: 0.8187 - val_loss: 0.9096 - val_acc: 0.7660\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7939 - acc: 0.8181 - val_loss: 0.9019 - val_acc: 0.7640\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7885 - acc: 0.8217 - val_loss: 0.9454 - val_acc: 0.7570\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8090 - acc: 0.8159 - val_loss: 0.9730 - val_acc: 0.7420\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8036 - acc: 0.8125 - val_loss: 0.9267 - val_acc: 0.7570\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7948 - acc: 0.8183 - val_loss: 0.9269 - val_acc: 0.7600\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7957 - acc: 0.8167 - val_loss: 0.9201 - val_acc: 0.7610\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7955 - acc: 0.8183 - val_loss: 0.9436 - val_acc: 0.7440\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8033 - acc: 0.8139 - val_loss: 1.1366 - val_acc: 0.6880\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8420 - acc: 0.8013 - val_loss: 0.9917 - val_acc: 0.7530\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7955 - acc: 0.8187 - val_loss: 0.9274 - val_acc: 0.7560\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8030 - acc: 0.8168 - val_loss: 0.9143 - val_acc: 0.7640\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8101 - acc: 0.8119 - val_loss: 0.9360 - val_acc: 0.7560\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7913 - acc: 0.8212 - val_loss: 0.9338 - val_acc: 0.7600\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8021 - acc: 0.8199 - val_loss: 0.9233 - val_acc: 0.7560\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7928 - acc: 0.8196 - val_loss: 1.1576 - val_acc: 0.7100\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8094 - acc: 0.8155 - val_loss: 0.9547 - val_acc: 0.7450\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7924 - acc: 0.8204 - val_loss: 0.9104 - val_acc: 0.7580\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8115 - acc: 0.8119 - val_loss: 0.9229 - val_acc: 0.7590\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7920 - acc: 0.8204 - val_loss: 0.9234 - val_acc: 0.7580\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8140 - acc: 0.8160 - val_loss: 0.9382 - val_acc: 0.7460\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7965 - acc: 0.8161 - val_loss: 0.9523 - val_acc: 0.7670\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8038 - acc: 0.8168 - val_loss: 0.9210 - val_acc: 0.7540\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7932 - acc: 0.8175 - val_loss: 0.9506 - val_acc: 0.7630\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7998 - acc: 0.8135 - val_loss: 0.9075 - val_acc: 0.7630\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8037 - acc: 0.8161 - val_loss: 0.9060 - val_acc: 0.7690\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7851 - acc: 0.8252 - val_loss: 0.9405 - val_acc: 0.7580\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7962 - acc: 0.8192 - val_loss: 0.9176 - val_acc: 0.7580\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8002 - acc: 0.8157 - val_loss: 0.9132 - val_acc: 0.7630\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7930 - acc: 0.8225 - val_loss: 0.9020 - val_acc: 0.7630\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7966 - acc: 0.8171 - val_loss: 0.9419 - val_acc: 0.7530\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7850 - acc: 0.8256 - val_loss: 0.9171 - val_acc: 0.7620\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8146 - acc: 0.8120 - val_loss: 0.9102 - val_acc: 0.7610\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7896 - acc: 0.8215 - val_loss: 0.9274 - val_acc: 0.7690\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8037 - acc: 0.8176 - val_loss: 0.9211 - val_acc: 0.7510\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7955 - acc: 0.8161 - val_loss: 0.9399 - val_acc: 0.7640\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7843 - acc: 0.8233 - val_loss: 0.9226 - val_acc: 0.7590\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7919 - acc: 0.8184 - val_loss: 1.0587 - val_acc: 0.7330\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8422 - acc: 0.7969 - val_loss: 0.9033 - val_acc: 0.7680\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7864 - acc: 0.8236 - val_loss: 0.9114 - val_acc: 0.7690\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8000 - acc: 0.8172 - val_loss: 0.9215 - val_acc: 0.7740\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8092 - acc: 0.8112 - val_loss: 0.9441 - val_acc: 0.7540\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8046 - acc: 0.8157 - val_loss: 1.0085 - val_acc: 0.7330\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8022 - acc: 0.8167 - val_loss: 0.9189 - val_acc: 0.7580\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7900 - acc: 0.8191 - val_loss: 0.9524 - val_acc: 0.7540\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7920 - acc: 0.8176 - val_loss: 0.9136 - val_acc: 0.7580\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7885 - acc: 0.8217 - val_loss: 0.9027 - val_acc: 0.7660\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7839 - acc: 0.8261 - val_loss: 0.9142 - val_acc: 0.7720\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8010 - acc: 0.8189 - val_loss: 0.9347 - val_acc: 0.7490\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7842 - acc: 0.8248 - val_loss: 0.9039 - val_acc: 0.7610\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8032 - acc: 0.8116 - val_loss: 0.9094 - val_acc: 0.7610\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7951 - acc: 0.8220 - val_loss: 0.9200 - val_acc: 0.7620\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7876 - acc: 0.8215 - val_loss: 0.9671 - val_acc: 0.7530\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8783 - acc: 0.7849 - val_loss: 0.9709 - val_acc: 0.7500\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7882 - acc: 0.8248 - val_loss: 1.1581 - val_acc: 0.6820\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8144 - acc: 0.8119 - val_loss: 0.9245 - val_acc: 0.7600\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7954 - acc: 0.8207 - val_loss: 0.9238 - val_acc: 0.7590\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7875 - acc: 0.8225 - val_loss: 0.9182 - val_acc: 0.7620\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7829 - acc: 0.8236 - val_loss: 0.9089 - val_acc: 0.7680\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7846 - acc: 0.8231 - val_loss: 1.0911 - val_acc: 0.7220\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8179 - acc: 0.8116 - val_loss: 0.9242 - val_acc: 0.7700\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7930 - acc: 0.8164 - val_loss: 1.0087 - val_acc: 0.7420\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7871 - acc: 0.8197 - val_loss: 0.9144 - val_acc: 0.7680\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8049 - acc: 0.8137 - val_loss: 0.9175 - val_acc: 0.7700\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7853 - acc: 0.8240 - val_loss: 0.9082 - val_acc: 0.7620\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7916 - acc: 0.8203 - val_loss: 0.9214 - val_acc: 0.7590\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7973 - acc: 0.8188 - val_loss: 0.9009 - val_acc: 0.7640\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7927 - acc: 0.8219 - val_loss: 0.9615 - val_acc: 0.7530\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7994 - acc: 0.8171 - val_loss: 0.9004 - val_acc: 0.7570\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7979 - acc: 0.8189 - val_loss: 0.9199 - val_acc: 0.7580\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7943 - acc: 0.8196 - val_loss: 0.9905 - val_acc: 0.7490\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8167 - acc: 0.8093 - val_loss: 1.5880 - val_acc: 0.6480\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8168 - acc: 0.8140 - val_loss: 0.9176 - val_acc: 0.7540\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7821 - acc: 0.8265 - val_loss: 0.9091 - val_acc: 0.7670\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7852 - acc: 0.8247 - val_loss: 0.9208 - val_acc: 0.7560\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7941 - acc: 0.8169 - val_loss: 0.9092 - val_acc: 0.7660\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8455 - acc: 0.7983 - val_loss: 0.9874 - val_acc: 0.7370\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7892 - acc: 0.8244 - val_loss: 0.9400 - val_acc: 0.7510\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7849 - acc: 0.8255 - val_loss: 0.9162 - val_acc: 0.7660\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7855 - acc: 0.8220 - val_loss: 0.9134 - val_acc: 0.7590\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7830 - acc: 0.8291 - val_loss: 0.9042 - val_acc: 0.7630\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7917 - acc: 0.8173 - val_loss: 0.9010 - val_acc: 0.7640\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7797 - acc: 0.8276 - val_loss: 0.9442 - val_acc: 0.7510\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7847 - acc: 0.8212 - val_loss: 0.9102 - val_acc: 0.7760\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7791 - acc: 0.8277 - val_loss: 0.9079 - val_acc: 0.7620\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7778 - acc: 0.8255 - val_loss: 0.9028 - val_acc: 0.7610\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7890 - acc: 0.8216 - val_loss: 0.9158 - val_acc: 0.7710\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7844 - acc: 0.8224 - val_loss: 0.8988 - val_acc: 0.7730\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7853 - acc: 0.8276 - val_loss: 0.9507 - val_acc: 0.7450\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7863 - acc: 0.8205 - val_loss: 0.9134 - val_acc: 0.7780\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8069 - acc: 0.8131 - val_loss: 0.9086 - val_acc: 0.7610\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7859 - acc: 0.8259 - val_loss: 0.9918 - val_acc: 0.7470\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8279 - acc: 0.8028 - val_loss: 0.9131 - val_acc: 0.7590\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7809 - acc: 0.8261 - val_loss: 0.9562 - val_acc: 0.7540\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7812 - acc: 0.8265 - val_loss: 0.9314 - val_acc: 0.7710\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7913 - acc: 0.8241 - val_loss: 0.9148 - val_acc: 0.7630\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8002 - acc: 0.8183 - val_loss: 0.9021 - val_acc: 0.7690\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7900 - acc: 0.8216 - val_loss: 0.9164 - val_acc: 0.7600\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7881 - acc: 0.8216 - val_loss: 0.9041 - val_acc: 0.7540\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7904 - acc: 0.8227 - val_loss: 0.9143 - val_acc: 0.7650\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7765 - acc: 0.8279 - val_loss: 0.9087 - val_acc: 0.7650\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7782 - acc: 0.8281 - val_loss: 0.9135 - val_acc: 0.7600\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7893 - acc: 0.8180 - val_loss: 0.9042 - val_acc: 0.7680\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7840 - acc: 0.8225 - val_loss: 0.9555 - val_acc: 0.7460\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8043 - acc: 0.8113 - val_loss: 0.9251 - val_acc: 0.7720\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7939 - acc: 0.8200 - val_loss: 0.9339 - val_acc: 0.7590\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8089 - acc: 0.8139 - val_loss: 0.9544 - val_acc: 0.7390\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7848 - acc: 0.8203 - val_loss: 0.9031 - val_acc: 0.7630\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8623 - acc: 0.8001 - val_loss: 0.9309 - val_acc: 0.7610\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8027 - acc: 0.8132 - val_loss: 0.9073 - val_acc: 0.7670\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7806 - acc: 0.8248 - val_loss: 0.9125 - val_acc: 0.7550\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7762 - acc: 0.8280 - val_loss: 0.9239 - val_acc: 0.7710\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8173 - acc: 0.8101 - val_loss: 0.9085 - val_acc: 0.7710\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7823 - acc: 0.8255 - val_loss: 0.9283 - val_acc: 0.7520\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7973 - acc: 0.8209 - val_loss: 0.9594 - val_acc: 0.7490\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7784 - acc: 0.8247 - val_loss: 0.9241 - val_acc: 0.7630\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7836 - acc: 0.8216 - val_loss: 0.9156 - val_acc: 0.7570\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7834 - acc: 0.8219 - val_loss: 0.9409 - val_acc: 0.7570\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7808 - acc: 0.8281 - val_loss: 0.9493 - val_acc: 0.7550\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7964 - acc: 0.8191 - val_loss: 0.9243 - val_acc: 0.7620\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.7844 - acc: 0.8248 - val_loss: 0.9385 - val_acc: 0.7550\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7802 - acc: 0.8240 - val_loss: 0.9340 - val_acc: 0.7490\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7955 - acc: 0.8228 - val_loss: 1.0226 - val_acc: 0.7280\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8143 - acc: 0.8095 - val_loss: 0.9578 - val_acc: 0.7770\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7884 - acc: 0.8173 - val_loss: 1.0138 - val_acc: 0.7380\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8067 - acc: 0.8140 - val_loss: 0.9130 - val_acc: 0.7600\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7822 - acc: 0.8259 - val_loss: 0.9202 - val_acc: 0.7590\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8240 - acc: 0.8088 - val_loss: 1.4409 - val_acc: 0.6510\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8171 - acc: 0.8049 - val_loss: 0.9150 - val_acc: 0.7670\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7766 - acc: 0.8323 - val_loss: 0.9201 - val_acc: 0.7730\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7788 - acc: 0.8265 - val_loss: 0.9009 - val_acc: 0.7640\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8169 - acc: 0.8137 - val_loss: 0.9477 - val_acc: 0.7680\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7797 - acc: 0.8261 - val_loss: 0.9281 - val_acc: 0.7790\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7851 - acc: 0.8227 - val_loss: 0.9467 - val_acc: 0.7510\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8045 - acc: 0.8140 - val_loss: 0.9077 - val_acc: 0.7670\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7934 - acc: 0.8203 - val_loss: 0.8992 - val_acc: 0.7700\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7755 - acc: 0.8259 - val_loss: 0.9049 - val_acc: 0.7640\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7888 - acc: 0.8223 - val_loss: 1.0979 - val_acc: 0.7220\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7908 - acc: 0.8195 - val_loss: 0.9303 - val_acc: 0.7530\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7874 - acc: 0.8231 - val_loss: 0.9478 - val_acc: 0.7500\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7861 - acc: 0.8208 - val_loss: 0.9255 - val_acc: 0.7740\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8198 - acc: 0.8073 - val_loss: 0.9106 - val_acc: 0.7690\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7830 - acc: 0.8241 - val_loss: 0.9270 - val_acc: 0.7550\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7914 - acc: 0.8213 - val_loss: 0.9178 - val_acc: 0.7630\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8008 - acc: 0.8187 - val_loss: 0.9520 - val_acc: 0.7600\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7818 - acc: 0.8257 - val_loss: 0.9553 - val_acc: 0.7530\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7743 - acc: 0.8283 - val_loss: 0.9486 - val_acc: 0.7600\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7876 - acc: 0.8255 - val_loss: 0.9306 - val_acc: 0.7600\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8645 - acc: 0.7992 - val_loss: 1.1100 - val_acc: 0.7220\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8054 - acc: 0.8159 - val_loss: 0.9569 - val_acc: 0.7580\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7745 - acc: 0.8287 - val_loss: 0.9323 - val_acc: 0.7620\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7731 - acc: 0.8289 - val_loss: 0.9170 - val_acc: 0.7580\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8098 - acc: 0.8160 - val_loss: 0.9301 - val_acc: 0.7520\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7807 - acc: 0.8285 - val_loss: 0.9058 - val_acc: 0.7600\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7802 - acc: 0.8259 - val_loss: 0.9261 - val_acc: 0.7700\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7896 - acc: 0.8240 - val_loss: 0.9416 - val_acc: 0.7690\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7726 - acc: 0.8324 - val_loss: 0.9420 - val_acc: 0.7580\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7792 - acc: 0.8268 - val_loss: 0.9416 - val_acc: 0.7540\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7728 - acc: 0.8295 - val_loss: 0.9220 - val_acc: 0.7730\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.7800 - acc: 0.8233 - val_loss: 0.9608 - val_acc: 0.7510\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8031 - acc: 0.8181 - val_loss: 0.9147 - val_acc: 0.7780\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7837 - acc: 0.8219 - val_loss: 0.9567 - val_acc: 0.7660\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7792 - acc: 0.8288 - val_loss: 0.9039 - val_acc: 0.7690\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7911 - acc: 0.8217 - val_loss: 0.9103 - val_acc: 0.7590\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7715 - acc: 0.8313 - val_loss: 0.9884 - val_acc: 0.7330\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8172 - acc: 0.8039 - val_loss: 1.1254 - val_acc: 0.7190\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7942 - acc: 0.8259 - val_loss: 0.9299 - val_acc: 0.7510\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7754 - acc: 0.8324 - val_loss: 0.9647 - val_acc: 0.7480\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8106 - acc: 0.8149 - val_loss: 0.9106 - val_acc: 0.7670\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8092 - acc: 0.8153 - val_loss: 0.9969 - val_acc: 0.7350\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7803 - acc: 0.8273 - val_loss: 0.9545 - val_acc: 0.7430\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7769 - acc: 0.8255 - val_loss: 0.9218 - val_acc: 0.7670\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7855 - acc: 0.8204 - val_loss: 0.9125 - val_acc: 0.7560\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 33us/step\n",
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7672582145690918, 0.8314666666348776]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9575415849685669, 0.7553333336512248]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.9764 - acc: 0.1612 - val_loss: 1.9422 - val_acc: 0.1970\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.9486 - acc: 0.1757 - val_loss: 1.9287 - val_acc: 0.2190\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9427 - acc: 0.1723 - val_loss: 1.9188 - val_acc: 0.2300\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9276 - acc: 0.1924 - val_loss: 1.9094 - val_acc: 0.2380\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.9140 - acc: 0.1949 - val_loss: 1.9004 - val_acc: 0.2470\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9060 - acc: 0.2105 - val_loss: 1.8907 - val_acc: 0.2510\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9054 - acc: 0.2148 - val_loss: 1.8821 - val_acc: 0.2610\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8932 - acc: 0.2239 - val_loss: 1.8718 - val_acc: 0.2640\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8843 - acc: 0.2356 - val_loss: 1.8609 - val_acc: 0.2720\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8753 - acc: 0.2255 - val_loss: 1.8491 - val_acc: 0.2780\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8660 - acc: 0.2388 - val_loss: 1.8372 - val_acc: 0.2930\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8534 - acc: 0.2564 - val_loss: 1.8236 - val_acc: 0.3150\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8515 - acc: 0.2559 - val_loss: 1.8087 - val_acc: 0.3320\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8299 - acc: 0.2723 - val_loss: 1.7914 - val_acc: 0.3460\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8153 - acc: 0.2773 - val_loss: 1.7719 - val_acc: 0.3540\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8039 - acc: 0.2873 - val_loss: 1.7524 - val_acc: 0.3720\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.7891 - acc: 0.2900 - val_loss: 1.7316 - val_acc: 0.3820\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.7816 - acc: 0.2984 - val_loss: 1.7112 - val_acc: 0.4050\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7681 - acc: 0.3060 - val_loss: 1.6890 - val_acc: 0.4170\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7532 - acc: 0.3176 - val_loss: 1.6665 - val_acc: 0.4420\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7345 - acc: 0.3241 - val_loss: 1.6425 - val_acc: 0.4600\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7134 - acc: 0.3328 - val_loss: 1.6175 - val_acc: 0.4700\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7035 - acc: 0.3457 - val_loss: 1.5945 - val_acc: 0.4960\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.6855 - acc: 0.3513 - val_loss: 1.5671 - val_acc: 0.5050\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6646 - acc: 0.3657 - val_loss: 1.5399 - val_acc: 0.5190\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6509 - acc: 0.3647 - val_loss: 1.5177 - val_acc: 0.5190\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6342 - acc: 0.3819 - val_loss: 1.4937 - val_acc: 0.5350\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6138 - acc: 0.3881 - val_loss: 1.4658 - val_acc: 0.5430\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5881 - acc: 0.4008 - val_loss: 1.4379 - val_acc: 0.5490\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5800 - acc: 0.3965 - val_loss: 1.4149 - val_acc: 0.5680\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5513 - acc: 0.4115 - val_loss: 1.3886 - val_acc: 0.5690\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5413 - acc: 0.4184 - val_loss: 1.3648 - val_acc: 0.5810\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5269 - acc: 0.4304 - val_loss: 1.3414 - val_acc: 0.5950\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.5132 - acc: 0.4231 - val_loss: 1.3202 - val_acc: 0.6040\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.4985 - acc: 0.4309 - val_loss: 1.2982 - val_acc: 0.6160\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.4738 - acc: 0.4508 - val_loss: 1.2773 - val_acc: 0.6280\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4679 - acc: 0.4515 - val_loss: 1.2584 - val_acc: 0.6280\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4369 - acc: 0.4625 - val_loss: 1.2300 - val_acc: 0.6490\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4309 - acc: 0.4624 - val_loss: 1.2121 - val_acc: 0.6510\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4223 - acc: 0.4780 - val_loss: 1.1936 - val_acc: 0.6610\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3896 - acc: 0.4867 - val_loss: 1.1701 - val_acc: 0.6650\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3808 - acc: 0.4799 - val_loss: 1.1526 - val_acc: 0.6710\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3675 - acc: 0.4937 - val_loss: 1.1360 - val_acc: 0.6810\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3428 - acc: 0.4983 - val_loss: 1.1166 - val_acc: 0.6910\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3452 - acc: 0.4968 - val_loss: 1.0994 - val_acc: 0.6970\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3266 - acc: 0.5049 - val_loss: 1.0841 - val_acc: 0.7030\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3163 - acc: 0.5071 - val_loss: 1.0706 - val_acc: 0.7040\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3017 - acc: 0.5185 - val_loss: 1.0550 - val_acc: 0.7140\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2855 - acc: 0.5207 - val_loss: 1.0396 - val_acc: 0.7160\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2866 - acc: 0.5217 - val_loss: 1.0284 - val_acc: 0.7120\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2704 - acc: 0.5261 - val_loss: 1.0158 - val_acc: 0.7130\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2626 - acc: 0.5212 - val_loss: 1.0017 - val_acc: 0.7240\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2491 - acc: 0.5351 - val_loss: 0.9937 - val_acc: 0.7170\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2389 - acc: 0.5409 - val_loss: 0.9802 - val_acc: 0.7220\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2330 - acc: 0.5353 - val_loss: 0.9684 - val_acc: 0.7280\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2082 - acc: 0.5533 - val_loss: 0.9533 - val_acc: 0.7230\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2110 - acc: 0.5569 - val_loss: 0.9422 - val_acc: 0.7280\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1953 - acc: 0.5524 - val_loss: 0.9325 - val_acc: 0.7300\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1655 - acc: 0.5640 - val_loss: 0.9192 - val_acc: 0.7300\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1694 - acc: 0.5713 - val_loss: 0.9074 - val_acc: 0.7370\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1645 - acc: 0.5724 - val_loss: 0.9011 - val_acc: 0.7430\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1561 - acc: 0.5677 - val_loss: 0.8929 - val_acc: 0.7370\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1522 - acc: 0.5821 - val_loss: 0.8846 - val_acc: 0.7370\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1442 - acc: 0.5727 - val_loss: 0.8754 - val_acc: 0.7450\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1172 - acc: 0.5808 - val_loss: 0.8641 - val_acc: 0.7460\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1210 - acc: 0.5865 - val_loss: 0.8556 - val_acc: 0.7520\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1138 - acc: 0.5849 - val_loss: 0.8478 - val_acc: 0.7480\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0975 - acc: 0.5928 - val_loss: 0.8380 - val_acc: 0.7520\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0982 - acc: 0.5927 - val_loss: 0.8320 - val_acc: 0.7560\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0786 - acc: 0.5928 - val_loss: 0.8247 - val_acc: 0.7540\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0786 - acc: 0.6015 - val_loss: 0.8194 - val_acc: 0.7580\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0663 - acc: 0.6133 - val_loss: 0.8091 - val_acc: 0.7590\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0786 - acc: 0.6079 - val_loss: 0.8044 - val_acc: 0.7610\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0587 - acc: 0.6047 - val_loss: 0.7980 - val_acc: 0.7600\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0606 - acc: 0.6099 - val_loss: 0.7926 - val_acc: 0.7610\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0449 - acc: 0.6128 - val_loss: 0.7876 - val_acc: 0.7620\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0497 - acc: 0.6139 - val_loss: 0.7833 - val_acc: 0.7590\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0406 - acc: 0.6185 - val_loss: 0.7797 - val_acc: 0.7600\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0349 - acc: 0.6169 - val_loss: 0.7718 - val_acc: 0.7600\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0265 - acc: 0.6228 - val_loss: 0.7682 - val_acc: 0.7620\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0209 - acc: 0.6212 - val_loss: 0.7629 - val_acc: 0.7620\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0165 - acc: 0.6292 - val_loss: 0.7597 - val_acc: 0.7650\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0029 - acc: 0.6229 - val_loss: 0.7546 - val_acc: 0.7610\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0032 - acc: 0.6303 - val_loss: 0.7472 - val_acc: 0.7610\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9939 - acc: 0.6275 - val_loss: 0.7422 - val_acc: 0.7640\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9855 - acc: 0.6355 - val_loss: 0.7378 - val_acc: 0.7620\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9845 - acc: 0.6400 - val_loss: 0.7364 - val_acc: 0.7640\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9891 - acc: 0.6327 - val_loss: 0.7316 - val_acc: 0.7640\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9816 - acc: 0.6375 - val_loss: 0.7306 - val_acc: 0.7640\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9844 - acc: 0.6371 - val_loss: 0.7261 - val_acc: 0.7670\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9730 - acc: 0.6437 - val_loss: 0.7247 - val_acc: 0.7700\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9620 - acc: 0.6471 - val_loss: 0.7164 - val_acc: 0.7700\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9552 - acc: 0.6469 - val_loss: 0.7153 - val_acc: 0.7660\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9467 - acc: 0.6455 - val_loss: 0.7089 - val_acc: 0.7680\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9601 - acc: 0.6409 - val_loss: 0.7089 - val_acc: 0.7660\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9364 - acc: 0.6540 - val_loss: 0.7032 - val_acc: 0.7650\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9310 - acc: 0.6504 - val_loss: 0.6999 - val_acc: 0.7640\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9520 - acc: 0.6440 - val_loss: 0.6988 - val_acc: 0.7670\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9386 - acc: 0.6591 - val_loss: 0.6972 - val_acc: 0.7710\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9316 - acc: 0.6552 - val_loss: 0.6933 - val_acc: 0.7730\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9282 - acc: 0.6611 - val_loss: 0.6910 - val_acc: 0.7720\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9280 - acc: 0.6597 - val_loss: 0.6879 - val_acc: 0.7670\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9352 - acc: 0.6477 - val_loss: 0.6872 - val_acc: 0.7690\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9268 - acc: 0.6561 - val_loss: 0.6863 - val_acc: 0.7640\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9212 - acc: 0.6623 - val_loss: 0.6829 - val_acc: 0.7630\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9268 - acc: 0.6577 - val_loss: 0.6815 - val_acc: 0.7620\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8995 - acc: 0.6668 - val_loss: 0.6765 - val_acc: 0.7660\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8915 - acc: 0.6733 - val_loss: 0.6723 - val_acc: 0.7660\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8924 - acc: 0.6749 - val_loss: 0.6708 - val_acc: 0.7670\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8923 - acc: 0.6745 - val_loss: 0.6675 - val_acc: 0.7670\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9067 - acc: 0.6611 - val_loss: 0.6688 - val_acc: 0.7720\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8813 - acc: 0.6683 - val_loss: 0.6654 - val_acc: 0.7670\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8980 - acc: 0.6657 - val_loss: 0.6644 - val_acc: 0.7650\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8917 - acc: 0.6735 - val_loss: 0.6605 - val_acc: 0.7710\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9020 - acc: 0.6676 - val_loss: 0.6599 - val_acc: 0.7680\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.8781 - acc: 0.6741 - val_loss: 0.6618 - val_acc: 0.7710\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8812 - acc: 0.6743 - val_loss: 0.6575 - val_acc: 0.7680\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8732 - acc: 0.6805 - val_loss: 0.6552 - val_acc: 0.7720\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8593 - acc: 0.6808 - val_loss: 0.6518 - val_acc: 0.7680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8625 - acc: 0.6779 - val_loss: 0.6506 - val_acc: 0.7710\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8644 - acc: 0.6829 - val_loss: 0.6496 - val_acc: 0.7700\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8639 - acc: 0.6792 - val_loss: 0.6477 - val_acc: 0.7690\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8586 - acc: 0.6837 - val_loss: 0.6471 - val_acc: 0.7730\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8534 - acc: 0.6811 - val_loss: 0.6453 - val_acc: 0.7710\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8533 - acc: 0.6836 - val_loss: 0.6438 - val_acc: 0.7690\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8457 - acc: 0.6889 - val_loss: 0.6425 - val_acc: 0.7720\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8336 - acc: 0.6909 - val_loss: 0.6390 - val_acc: 0.7660\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8438 - acc: 0.6908 - val_loss: 0.6385 - val_acc: 0.7690\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8370 - acc: 0.6929 - val_loss: 0.6363 - val_acc: 0.7710\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8510 - acc: 0.6801 - val_loss: 0.6365 - val_acc: 0.7710\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8444 - acc: 0.6911 - val_loss: 0.6339 - val_acc: 0.7700\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8465 - acc: 0.6892 - val_loss: 0.6333 - val_acc: 0.7730\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8320 - acc: 0.6936 - val_loss: 0.6324 - val_acc: 0.7730\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8198 - acc: 0.6952 - val_loss: 0.6302 - val_acc: 0.7770\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8172 - acc: 0.6928 - val_loss: 0.6272 - val_acc: 0.7780\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8062 - acc: 0.7025 - val_loss: 0.6269 - val_acc: 0.7730\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8272 - acc: 0.6976 - val_loss: 0.6262 - val_acc: 0.7760\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8208 - acc: 0.6937 - val_loss: 0.6237 - val_acc: 0.7780\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8236 - acc: 0.6956 - val_loss: 0.6235 - val_acc: 0.7750\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8030 - acc: 0.7072 - val_loss: 0.6211 - val_acc: 0.7820\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8072 - acc: 0.7001 - val_loss: 0.6200 - val_acc: 0.7810\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8129 - acc: 0.6988 - val_loss: 0.6211 - val_acc: 0.7750\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7976 - acc: 0.7039 - val_loss: 0.6199 - val_acc: 0.7770\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8030 - acc: 0.7028 - val_loss: 0.6202 - val_acc: 0.7740\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8057 - acc: 0.7044 - val_loss: 0.6164 - val_acc: 0.7740\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7990 - acc: 0.6973 - val_loss: 0.6165 - val_acc: 0.7730\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8105 - acc: 0.6995 - val_loss: 0.6152 - val_acc: 0.7730\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7966 - acc: 0.7057 - val_loss: 0.6143 - val_acc: 0.7780\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7942 - acc: 0.7113 - val_loss: 0.6139 - val_acc: 0.7760\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7983 - acc: 0.6999 - val_loss: 0.6130 - val_acc: 0.7810\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7905 - acc: 0.7116 - val_loss: 0.6130 - val_acc: 0.7780\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7944 - acc: 0.7088 - val_loss: 0.6105 - val_acc: 0.7810\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7711 - acc: 0.7145 - val_loss: 0.6084 - val_acc: 0.7790\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7784 - acc: 0.7049 - val_loss: 0.6084 - val_acc: 0.7810\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7781 - acc: 0.7081 - val_loss: 0.6073 - val_acc: 0.7850\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7871 - acc: 0.7121 - val_loss: 0.6057 - val_acc: 0.7860\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7793 - acc: 0.7091 - val_loss: 0.6048 - val_acc: 0.7810\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7838 - acc: 0.7104 - val_loss: 0.6073 - val_acc: 0.7800\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7695 - acc: 0.7175 - val_loss: 0.6049 - val_acc: 0.7790\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7697 - acc: 0.7113 - val_loss: 0.6031 - val_acc: 0.7820\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7740 - acc: 0.7109 - val_loss: 0.6035 - val_acc: 0.7820\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7545 - acc: 0.7205 - val_loss: 0.6012 - val_acc: 0.7810\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7669 - acc: 0.7227 - val_loss: 0.5997 - val_acc: 0.7840\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7718 - acc: 0.7219 - val_loss: 0.5997 - val_acc: 0.7870\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7592 - acc: 0.7204 - val_loss: 0.5981 - val_acc: 0.7810\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7687 - acc: 0.7128 - val_loss: 0.5986 - val_acc: 0.7830\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7584 - acc: 0.7207 - val_loss: 0.5975 - val_acc: 0.7880\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7476 - acc: 0.7251 - val_loss: 0.5969 - val_acc: 0.7810\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7508 - acc: 0.7233 - val_loss: 0.5961 - val_acc: 0.7840\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7615 - acc: 0.7208 - val_loss: 0.5951 - val_acc: 0.7860\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7427 - acc: 0.7288 - val_loss: 0.5944 - val_acc: 0.7880\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7533 - acc: 0.7177 - val_loss: 0.5932 - val_acc: 0.7860\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7569 - acc: 0.7125 - val_loss: 0.5963 - val_acc: 0.7870\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7427 - acc: 0.7287 - val_loss: 0.5930 - val_acc: 0.7850\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7456 - acc: 0.7341 - val_loss: 0.5932 - val_acc: 0.7850\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7492 - acc: 0.7196 - val_loss: 0.5899 - val_acc: 0.7880\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7462 - acc: 0.7255 - val_loss: 0.5902 - val_acc: 0.7880\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7517 - acc: 0.7164 - val_loss: 0.5896 - val_acc: 0.7880\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7502 - acc: 0.7237 - val_loss: 0.5905 - val_acc: 0.7880\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7357 - acc: 0.7280 - val_loss: 0.5903 - val_acc: 0.7860\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7314 - acc: 0.7284 - val_loss: 0.5865 - val_acc: 0.7850\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7330 - acc: 0.7335 - val_loss: 0.5862 - val_acc: 0.7850\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7248 - acc: 0.7323 - val_loss: 0.5848 - val_acc: 0.7860\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7310 - acc: 0.7281 - val_loss: 0.5830 - val_acc: 0.7910\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.7434 - acc: 0.7211 - val_loss: 0.5846 - val_acc: 0.7870\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7211 - acc: 0.7303 - val_loss: 0.5819 - val_acc: 0.7940\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7414 - acc: 0.7257 - val_loss: 0.5852 - val_acc: 0.7910\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7152 - acc: 0.7324 - val_loss: 0.5842 - val_acc: 0.7900\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7140 - acc: 0.7365 - val_loss: 0.5813 - val_acc: 0.7910\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7264 - acc: 0.7325 - val_loss: 0.5805 - val_acc: 0.7910\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7160 - acc: 0.7339 - val_loss: 0.5792 - val_acc: 0.7890\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7273 - acc: 0.7356 - val_loss: 0.5784 - val_acc: 0.7910\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7097 - acc: 0.7372 - val_loss: 0.5791 - val_acc: 0.7900\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.7174 - acc: 0.7336 - val_loss: 0.5784 - val_acc: 0.7920\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7309 - acc: 0.7227 - val_loss: 0.5789 - val_acc: 0.7900\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7130 - acc: 0.7389 - val_loss: 0.5788 - val_acc: 0.7930\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7026 - acc: 0.7464 - val_loss: 0.5774 - val_acc: 0.7920\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7080 - acc: 0.7353 - val_loss: 0.5778 - val_acc: 0.7930\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7075 - acc: 0.7375 - val_loss: 0.5757 - val_acc: 0.7920\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7014 - acc: 0.7427 - val_loss: 0.5755 - val_acc: 0.7930\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 35us/step\n",
      "1500/1500 [==============================] - 0s 29us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42917041641871134, 0.8636000000317892]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6155493454933166, 0.7726666666666666]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 38us/step - loss: 1.9045 - acc: 0.2196 - val_loss: 1.8582 - val_acc: 0.2533\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 1.7907 - acc: 0.3141 - val_loss: 1.7335 - val_acc: 0.3713\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.6325 - acc: 0.4265 - val_loss: 1.5568 - val_acc: 0.4980\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 1.4419 - acc: 0.5328 - val_loss: 1.3652 - val_acc: 0.5753\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 1.2545 - acc: 0.6084 - val_loss: 1.1911 - val_acc: 0.6263\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 1.0941 - acc: 0.6537 - val_loss: 1.0521 - val_acc: 0.6607\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.9717 - acc: 0.6842 - val_loss: 0.9492 - val_acc: 0.6770\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.8835 - acc: 0.7009 - val_loss: 0.8757 - val_acc: 0.6910\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.8200 - acc: 0.7160 - val_loss: 0.8221 - val_acc: 0.7053\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.7726 - acc: 0.7285 - val_loss: 0.7814 - val_acc: 0.7157\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.7357 - acc: 0.7394 - val_loss: 0.7490 - val_acc: 0.7327\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.7069 - acc: 0.7488 - val_loss: 0.7238 - val_acc: 0.7350\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.6826 - acc: 0.7555 - val_loss: 0.7031 - val_acc: 0.7423\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.6624 - acc: 0.7623 - val_loss: 0.6843 - val_acc: 0.7463\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.6450 - acc: 0.7674 - val_loss: 0.6679 - val_acc: 0.7517\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.6297 - acc: 0.7727 - val_loss: 0.6557 - val_acc: 0.7520\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.6165 - acc: 0.7773 - val_loss: 0.6423 - val_acc: 0.7610\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.6042 - acc: 0.7804 - val_loss: 0.6339 - val_acc: 0.7613\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5935 - acc: 0.7857 - val_loss: 0.6239 - val_acc: 0.7670\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5830 - acc: 0.7886 - val_loss: 0.6165 - val_acc: 0.7730\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5737 - acc: 0.7922 - val_loss: 0.6085 - val_acc: 0.7730\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5654 - acc: 0.7947 - val_loss: 0.6010 - val_acc: 0.7737\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5573 - acc: 0.7982 - val_loss: 0.5947 - val_acc: 0.7780\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5496 - acc: 0.8020 - val_loss: 0.5909 - val_acc: 0.7783\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5425 - acc: 0.8048 - val_loss: 0.5838 - val_acc: 0.7833\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.5359 - acc: 0.8065 - val_loss: 0.5785 - val_acc: 0.7850\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.5295 - acc: 0.8103 - val_loss: 0.5737 - val_acc: 0.7867\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5235 - acc: 0.8128 - val_loss: 0.5699 - val_acc: 0.7920\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5177 - acc: 0.8147 - val_loss: 0.5676 - val_acc: 0.7927\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 0.5118 - acc: 0.8177 - val_loss: 0.5625 - val_acc: 0.7927\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5066 - acc: 0.8203 - val_loss: 0.5593 - val_acc: 0.7930\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.5018 - acc: 0.8221 - val_loss: 0.5563 - val_acc: 0.7927\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 22us/step - loss: 0.4971 - acc: 0.8234 - val_loss: 0.5540 - val_acc: 0.7940\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 21us/step - loss: 0.4921 - acc: 0.8255 - val_loss: 0.5499 - val_acc: 0.7987\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.4877 - acc: 0.8286 - val_loss: 0.5481 - val_acc: 0.7963\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.4832 - acc: 0.8296 - val_loss: 0.5454 - val_acc: 0.7993\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4788 - acc: 0.8303 - val_loss: 0.5451 - val_acc: 0.7963\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4751 - acc: 0.8325 - val_loss: 0.5407 - val_acc: 0.8007\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4712 - acc: 0.8347 - val_loss: 0.5399 - val_acc: 0.7983\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4678 - acc: 0.8355 - val_loss: 0.5387 - val_acc: 0.7970\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 20us/step - loss: 0.4639 - acc: 0.8371 - val_loss: 0.5357 - val_acc: 0.8003\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4604 - acc: 0.8380 - val_loss: 0.5374 - val_acc: 0.8000\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4567 - acc: 0.8401 - val_loss: 0.5356 - val_acc: 0.8013\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4535 - acc: 0.8408 - val_loss: 0.5308 - val_acc: 0.8050\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4502 - acc: 0.8421 - val_loss: 0.5309 - val_acc: 0.8020\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4473 - acc: 0.8441 - val_loss: 0.5293 - val_acc: 0.8043\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4440 - acc: 0.8444 - val_loss: 0.5289 - val_acc: 0.8050\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4410 - acc: 0.8456 - val_loss: 0.5284 - val_acc: 0.8047\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4382 - acc: 0.8465 - val_loss: 0.5279 - val_acc: 0.8063\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4353 - acc: 0.8471 - val_loss: 0.5248 - val_acc: 0.8067\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4327 - acc: 0.8488 - val_loss: 0.5268 - val_acc: 0.8060\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4299 - acc: 0.8499 - val_loss: 0.5242 - val_acc: 0.8070\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4273 - acc: 0.8502 - val_loss: 0.5236 - val_acc: 0.8083\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4246 - acc: 0.8517 - val_loss: 0.5253 - val_acc: 0.8100\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4221 - acc: 0.8518 - val_loss: 0.5227 - val_acc: 0.8083\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4199 - acc: 0.8533 - val_loss: 0.5237 - val_acc: 0.8047\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4176 - acc: 0.8534 - val_loss: 0.5221 - val_acc: 0.8073\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4150 - acc: 0.8548 - val_loss: 0.5229 - val_acc: 0.8057\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4127 - acc: 0.8553 - val_loss: 0.5225 - val_acc: 0.8087\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4104 - acc: 0.8563 - val_loss: 0.5227 - val_acc: 0.8080\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.4081 - acc: 0.8574 - val_loss: 0.5206 - val_acc: 0.8097\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4063 - acc: 0.8578 - val_loss: 0.5214 - val_acc: 0.8063\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.4041 - acc: 0.8587 - val_loss: 0.5238 - val_acc: 0.8073\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4020 - acc: 0.8591 - val_loss: 0.5227 - val_acc: 0.8137\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3998 - acc: 0.8590 - val_loss: 0.5218 - val_acc: 0.8090\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3978 - acc: 0.8602 - val_loss: 0.5218 - val_acc: 0.8127\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3959 - acc: 0.8610 - val_loss: 0.5209 - val_acc: 0.8067\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3936 - acc: 0.8625 - val_loss: 0.5213 - val_acc: 0.8067\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3921 - acc: 0.8633 - val_loss: 0.5217 - val_acc: 0.8073\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3899 - acc: 0.8635 - val_loss: 0.5214 - val_acc: 0.8073\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3882 - acc: 0.8643 - val_loss: 0.5224 - val_acc: 0.8060\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3866 - acc: 0.8652 - val_loss: 0.5221 - val_acc: 0.8077\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3843 - acc: 0.8655 - val_loss: 0.5232 - val_acc: 0.8060\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3828 - acc: 0.8668 - val_loss: 0.5238 - val_acc: 0.8067\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3808 - acc: 0.8671 - val_loss: 0.5228 - val_acc: 0.8090\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3794 - acc: 0.8682 - val_loss: 0.5254 - val_acc: 0.8050\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3777 - acc: 0.8684 - val_loss: 0.5245 - val_acc: 0.8103\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3758 - acc: 0.8683 - val_loss: 0.5248 - val_acc: 0.8083\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3745 - acc: 0.8687 - val_loss: 0.5254 - val_acc: 0.8070\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3727 - acc: 0.8698 - val_loss: 0.5258 - val_acc: 0.8073\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3710 - acc: 0.8708 - val_loss: 0.5270 - val_acc: 0.8057\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3696 - acc: 0.8704 - val_loss: 0.5254 - val_acc: 0.8067\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3678 - acc: 0.8721 - val_loss: 0.5280 - val_acc: 0.8077\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3663 - acc: 0.8723 - val_loss: 0.5277 - val_acc: 0.8063\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3645 - acc: 0.8725 - val_loss: 0.5296 - val_acc: 0.8043\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3634 - acc: 0.8738 - val_loss: 0.5292 - val_acc: 0.8060\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3620 - acc: 0.8741 - val_loss: 0.5312 - val_acc: 0.8060\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3606 - acc: 0.8736 - val_loss: 0.5310 - val_acc: 0.8073\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3589 - acc: 0.8740 - val_loss: 0.5323 - val_acc: 0.8077\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3572 - acc: 0.8754 - val_loss: 0.5337 - val_acc: 0.8047\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3563 - acc: 0.8752 - val_loss: 0.5332 - val_acc: 0.8060\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3547 - acc: 0.8759 - val_loss: 0.5334 - val_acc: 0.8027\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3531 - acc: 0.8759 - val_loss: 0.5340 - val_acc: 0.8020\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3519 - acc: 0.8772 - val_loss: 0.5351 - val_acc: 0.8067\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3504 - acc: 0.8771 - val_loss: 0.5365 - val_acc: 0.8067\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3492 - acc: 0.8767 - val_loss: 0.5383 - val_acc: 0.8087\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3474 - acc: 0.8780 - val_loss: 0.5416 - val_acc: 0.7993\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3467 - acc: 0.8788 - val_loss: 0.5378 - val_acc: 0.8047\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3453 - acc: 0.8794 - val_loss: 0.5384 - val_acc: 0.8030\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3438 - acc: 0.8795 - val_loss: 0.5404 - val_acc: 0.8060\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3427 - acc: 0.8805 - val_loss: 0.5403 - val_acc: 0.8020\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3414 - acc: 0.8804 - val_loss: 0.5416 - val_acc: 0.8040\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3401 - acc: 0.8816 - val_loss: 0.5454 - val_acc: 0.8067\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3390 - acc: 0.8815 - val_loss: 0.5456 - val_acc: 0.8050\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3379 - acc: 0.8812 - val_loss: 0.5436 - val_acc: 0.8067\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3364 - acc: 0.8821 - val_loss: 0.5462 - val_acc: 0.8023\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3354 - acc: 0.8823 - val_loss: 0.5477 - val_acc: 0.8040\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3341 - acc: 0.8834 - val_loss: 0.5466 - val_acc: 0.8043\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3329 - acc: 0.8831 - val_loss: 0.5511 - val_acc: 0.8070\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3317 - acc: 0.8834 - val_loss: 0.5484 - val_acc: 0.8023\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3304 - acc: 0.8843 - val_loss: 0.5510 - val_acc: 0.8043\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3294 - acc: 0.8850 - val_loss: 0.5506 - val_acc: 0.8037\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3280 - acc: 0.8851 - val_loss: 0.5520 - val_acc: 0.8043\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3271 - acc: 0.8850 - val_loss: 0.5524 - val_acc: 0.8047\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3261 - acc: 0.8861 - val_loss: 0.5537 - val_acc: 0.8043\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3247 - acc: 0.8865 - val_loss: 0.5568 - val_acc: 0.8067\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3235 - acc: 0.8864 - val_loss: 0.5589 - val_acc: 0.8060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3223 - acc: 0.8869 - val_loss: 0.5567 - val_acc: 0.8047\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3213 - acc: 0.8874 - val_loss: 0.5621 - val_acc: 0.8043\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3202 - acc: 0.8877 - val_loss: 0.5617 - val_acc: 0.8033\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 25us/step\n",
      "4000/4000 [==============================] - 0s 49us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31786946030096574, 0.8888787878787878]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5722052040100097, 0.805]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
